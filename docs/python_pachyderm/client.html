<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>python_pachyderm.client API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>python_pachyderm.client</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>Source code</summary>
<pre><code class="python">import os
import collections
import itertools
from contextlib import contextmanager, closing

from python_pachyderm.proto.pfs import pfs_pb2 as pfs_proto
from python_pachyderm.proto.pfs import pfs_pb2_grpc as pfs_grpc
from python_pachyderm.proto.pps import pps_pb2 as pps_proto
from python_pachyderm.proto.pps import pps_pb2_grpc as pps_grpc
from python_pachyderm.proto.version.versionpb import version_pb2_grpc as version_grpc


BUFFER_SIZE = 3 * 1024 * 1024  # 3MB TODO: Base this on some grpc value


def commit_from(src, allow_just_repo=False):
    if isinstance(src, pfs_proto.Commit):
        return src
    elif isinstance(src, (tuple, list)) and len(src) == 2:
        return pfs_proto.Commit(repo=pfs_proto.Repo(name=src[0]), id=src[1])
    elif isinstance(src, str):
        repo_name, commit_id = src.split(&#39;/&#39;, 1)
        return pfs_proto.Commit(repo=pfs_proto.Repo(name=repo_name), id=commit_id)

    if not allow_just_repo:
        raise ValueError(&#34;Invalid commit type&#34;)
    return pfs_proto.Commit(repo=pfs_proto.Repo(name=src))


class Client(object):
    def __init__(self, host=None, port=None, auth_token=None, root_certs=None):
        &#34;&#34;&#34;
        Creates a client to connect to PFS.

        Params:

        * host: The pachd host. Default is &#39;localhost&#39;, which is used with
        `pachctl port-forward`.
        * port: The port to connect to. Default is 30650.
        * auth_token: The authentication token; used if authentication is
        enabled on the cluster. Default to `None`.
        * root_certs:  The PEM-encoded root certificates as byte string.
        &#34;&#34;&#34;

        if host is not None and port is not None:
            self.address = &#34;{}:{}&#34;.format(host, port)
        else:
            self.address = os.environ.get(&#34;PACHD_ADDRESS&#34;, &#34;localhost:30650&#34;)

        if auth_token is None:
            auth_token = os.environ.get(&#34;PACH_PYTHON_AUTH_TOKEN&#34;)

        self.metadata = []
        if auth_token is not None:
            self.metadata.append((&#34;authn-token&#34;, auth_token))

        self.root_certs = root_certs

    @property
    def _pfs_stub(self):
        if not hasattr(self, &#34;__pfs_stub&#34;):
            if self.root_certs:
                ssl_channel_credentials = pfs_grpc.grpc.ssl_channel_credentials
                ssl = ssl_channel_credentials(root_certificates=root_certs)
                channel = pfs_grpc.grpc.secure_channel(self.address, ssl)
            else:
                channel = pfs_grpc.grpc.insecure_channel(self.address)
            self.__pfs_stub = pfs_grpc.APIStub(channel)
        return self.__pfs_stub

    @property
    def _pps_stub(self):
        if not hasattr(self, &#34;__pps_stub&#34;):
            if self.root_certs:
                ssl_channel_credentials = pps_grpc.grpc.ssl_channel_credentials
                ssl = ssl_channel_credentials(root_certificates=root_certs)
                channel = pps_grpc.grpc.secure_channel(self.address, ssl)
            else:
                channel = pps_grpc.grpc.insecure_channel(self.address)
            self.__pps_stub = pps_grpc.APIStub(channel)
        return self.__pps_stub

    def get_remote_version(self):
        with closing(version_grpc.grpc.insecure_channel(self.address)) as channel:
            stub = version_grpc.APIStub(channel)
            return stub.GetVersion(version_grpc.google_dot_protobuf_dot_empty__pb2.Empty())

    def create_repo(self, repo_name, description=None, update=None):
        &#34;&#34;&#34;
        Creates a new `Repo` object in PFS with the given name. Repos are the
        top level data object in PFS and should be used to store data of a
        similar type. For example rather than having a single `Repo` for an
        entire project you might have separate `Repo`s for logs, metrics,
        database dumps etc.

        Params:

        * repo_name: Name of the repo.
        * description: An optional string describing the repo.
        * update: Whether to update if the repo already exists.
        &#34;&#34;&#34;
        req = pfs_proto.CreateRepoRequest(
            repo=pfs_proto.Repo(name=repo_name),
            description=description,
            update=update
        )
        self._pfs_stub.CreateRepo(req, metadata=self.metadata)

    def inspect_repo(self, repo_name):
        &#34;&#34;&#34;
        Returns info about a specific repo. Returns a `RepoInfo` object.

        Params:
        * repo_name: Name of the repo.
        &#34;&#34;&#34;
        req = pfs_proto.InspectRepoRequest(repo=pfs_proto.Repo(name=repo_name))
        return self._pfs_stub.InspectRepo(req, metadata=self.metadata)

    def list_repo(self):
        &#34;&#34;&#34;
        Returns info about all repos, as a list of `RepoInfo` objects.
        &#34;&#34;&#34;
        req = pfs_proto.ListRepoRequest()
        res = self._pfs_stub.ListRepo(req, metadata=self.metadata)
        return res.repo_info

    def delete_repo(self, repo_name, force=None):
        &#34;&#34;&#34;
        Deletes a repo and reclaims the storage space it was using.

        Params:

        * repo_name: The name of the repo.
        * force: If set to true, the repo will be removed regardless of
        errors. This argument should be used with care.
        &#34;&#34;&#34;
        req = pfs_proto.DeleteRepoRequest(repo=pfs_proto.Repo(name=repo_name), force=force, all=False)
        self._pfs_stub.DeleteRepo(req, metadata=self.metadata)

    def delete_all_repos(self, force=None):
        &#34;&#34;&#34;
        Deletes all repos.

        Params:

        * force: If set to true, the repo will be removed regardless of
        errors. This argument should be used with care.
        &#34;&#34;&#34;

        req = pfs_proto.DeleteRepoRequest(force=force, all=True)
        self._pfs_stub.DeleteRepo(req, metadata=self.metadata)

    def start_commit(self, repo_name, branch=None, parent=None, description=None, provenance=None):
        &#34;&#34;&#34;
        Begins the process of committing data to a Repo. Once started you can
        write to the Commit with PutFile and when all the data has been
        written you must finish the Commit with FinishCommit. NOTE, data is
        not persisted until FinishCommit is called. A Commit object is
        returned.

        Params:

        * repo_name: A string specifying the name of the repo.
        * branch: A string specifying the branch name. This is a more
        convenient way to build linear chains of commits. When a commit is
        started with a non-empty branch the value of branch becomes an alias
        for the created Commit. This enables a more intuitive access pattern.
        When the commit is started on a branch the previous head of the branch
        is used as the parent of the commit.
        * parent: An optional `Commit` object specifying the parent commit.
        Upon creation the new commit will appear identical to the parent
        commit, data can safely be added to the new commit without affecting
        the contents of the parent commit.
        * description: An optional string describing the commit.
        * provenance: An optional iterable of `CommitProvenance` objects
        specifying the commit provenance.
        &#34;&#34;&#34;
        req = pfs_proto.StartCommitRequest(
            parent=pfs_proto.Commit(repo=pfs_proto.Repo(name=repo_name), id=parent),
            branch=branch,
            description=description,
            provenance=provenance,
        )
        return self._pfs_stub.StartCommit(req, metadata=self.metadata)

    def finish_commit(self, commit, description=None,
                      tree_object_hashes=None, datum_object_hash=None,
                      size_bytes=None, empty=None):
        &#34;&#34;&#34;
        Ends the process of committing data to a Repo and persists the
        Commit. Once a Commit is finished the data becomes immutable and
        future attempts to write to it with PutFile will error.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * description: An optional string describing this commit.
        * tree_object_hashes: A list of zero or more strings specifying object
        hashes.
        * datum_object_hash: An optional string specifying an object hash.
        * size_bytes: An optional int.
        * empty: An optional bool. If set, the commit will be closed (its
        `finished` field will be set to the current time) but its `tree` will
        be left nil.
        &#34;&#34;&#34;
        req = pfs_proto.FinishCommitRequest(
            commit=commit_from(commit),
            description=description,
            trees=[pfs_proto.Object(hash=h) for h in tree_object_hashes] if tree_object_hashes is not None else None,
            datums=pfs_proto.Object(hash=datum_object_hash) if datum_object_hash is not None else None,
            size_bytes=size_bytes,
            empty=empty,
        )
        return self._pfs_stub.FinishCommit(req, metadata=self.metadata)

    @contextmanager
    def commit(self, repo_name, branch=None, parent=None, description=None):
        &#34;&#34;&#34;
        A context manager for running operations within a commit.

        Params:

        * repo_name: A string specifying the name of the repo.
        * branch: A string specifying the branch name. This is a more
        convenient way to build linear chains of commits. When a commit is
        started with a non-empty branch the value of branch becomes an alias
        for the created Commit. This enables a more intuitive access pattern.
        When the commit is started on a branch the previous head of the branch
        is used as the parent of the commit.
        * parent: An optional `Commit` object specifying the parent commit.
        Upon creation the new commit will appear identical to the parent
        commit, data can safely be added to the new commit without affecting
        the contents of the parent commit.
        * description: An optional string describing the commit.
        &#34;&#34;&#34;
        commit = self.start_commit(repo_name, branch, parent, description)
        try:
            yield commit
        finally:
            self.finish_commit(commit)

    def inspect_commit(self, commit, block_state=None):
        &#34;&#34;&#34;
        Inspects a commit. Returns a `CommitInfo` object.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * block_state: Causes inspect commit to block until the commit is in
        the desired commit state.
        &#34;&#34;&#34;
        req = pfs_proto.InspectCommitRequest(commit=commit_from(commit), block_state=block_state)
        return self._pfs_stub.InspectCommit(req, metadata=self.metadata)

    def list_commit(self, repo_name, to_commit=None, from_commit=None, number=None):
        &#34;&#34;&#34;
        Lists commits. Yields `CommitInfo` objects.

        Params:

        * repo_name: If only `repo_name` is given, all commits in the repo are
        returned.
        * to_commit: Optional. Only the ancestors of `to`, including `to`
        itself, are considered.
        * from_commit: Optional. Only the descendants of `from`, including
        `from` itself, are considered.
        * number: Optional. Determines how many commits are returned.  If
        `number` is 0, all commits that match the aforementioned criteria are
        returned.
        &#34;&#34;&#34;
        req = pfs_proto.ListCommitRequest(repo=pfs_proto.Repo(name=repo_name), number=number)
        if to_commit is not None:
            req.to.CopyFrom(commit_from(to_commit))
        if from_commit is not None:
            getattr(req, &#39;from&#39;).CopyFrom(commit_from(from_commit))
        return self._pfs_stub.ListCommitStream(req, metadata=self.metadata)

    def delete_commit(self, commit):
        &#34;&#34;&#34;
        Deletes a commit.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        &#34;&#34;&#34;
        req = pfs_proto.DeleteCommitRequest(commit=commit_from(commit))
        self._pfs_stub.DeleteCommit(req, metadata=self.metadata)

    def flush_commit(self, commits, repos=None):
        &#34;&#34;&#34;
        Blocks until all of the commits which have a set of commits as
        provenance have finished. For commits to be considered they must have
        all of the specified commits as provenance. This in effect waits for
        all of the jobs that are triggered by a set of commits to complete.
        It returns an error if any of the commits it&#39;s waiting on are
        cancelled due to one of the jobs encountering an error during runtime.
        Note that it&#39;s never necessary to call FlushCommit to run jobs,
        they&#39;ll run no matter what, FlushCommit just allows you to wait for
        them to complete and see their output once they do. This returns an
        iterator of CommitInfo objects.

        Yields `CommitInfo` objects.

        Params:

        * commits: A list of tuples, strings, or `Commit` objects representing
        the commits to flush.
        * repos: An optional list of strings specifying repo names. If
        specified, only commits within these repos will be flushed.
        &#34;&#34;&#34;
        to_repos = [pfs_proto.Repo(name=r) for r in repos] if repos is not None else None
        req = pfs_proto.FlushCommitRequest(commits=[commit_from(c) for c in commits],
                                       to_repos=to_repos)
        return self._pfs_stub.FlushCommit(req, metadata=self.metadata)

    def subscribe_commit(self, repo_name, branch, from_commit_id=None, state=None):
        &#34;&#34;&#34;
        Yields `CommitInfo` objects as commits occur.

        Params:

        * repo_name: A string specifying the name of the repo.
        * branch: A string specifying branch to subscribe to.
        * from_commit_id: An optional string specifying the commit ID. Only
        commits created since this commit are returned.
        * state: The commit state to filter on.
        &#34;&#34;&#34;
        repo = pfs_proto.Repo(name=repo_name)
        req = pfs_proto.SubscribeCommitRequest(repo=repo, branch=branch, state=state)
        if from_commit_id is not None:
            getattr(req, &#39;from&#39;).CopyFrom(pfs_proto.Commit(repo=repo, id=from_commit_id))
        return self._pfs_stub.SubscribeCommit(req, metadata=self.metadata)

    def create_branch(self, repo_name, branch_name, commit=None, provenance=None):
        &#34;&#34;&#34;
        Creates a new branch.

        Params:
        * repo_name: A string specifying the name of the repo.
        * branch_name: A string specifying the new branch name.
        * commit: An optional tuple, string, or `Commit` object representing
        the head commit of the branch.
        * provenance: An optional iterable of `Branch` objects representing
        the branch provenance.
        &#34;&#34;&#34;
        req = pfs_proto.CreateBranchRequest(
            branch=pfs_proto.Branch(repo=pfs_proto.Repo(name=repo_name), name=branch_name),
            head=commit_from(commit) if commit is not None else None,
            provenance=provenance,
        )
        self._pfs_stub.CreateBranch(req, metadata=self.metadata)

    def inspect_branch(self, repo_name, branch_name):
        &#34;&#34;&#34;
        Inspects a branch. Returns a `BranchInfo` object.
        &#34;&#34;&#34;
        branch = pfs_proto.Branch(repo=pfs_proto.Repo(name=repo_name), name=branch_name)
        req = pfs_proto.InspectBranchRequest(branch=branch)
        return self._pfs_stub.InspectBranch(req, metadata=self.metadata)

    def list_branch(self, repo_name):
        &#34;&#34;&#34;
        Lists the active branch objects on a repo. Returns a list of
        `BranchInfo` objects.

        Params:

        * repo_name: A string specifying the repo name.
        &#34;&#34;&#34;
        req = pfs_proto.ListBranchRequest(repo=pfs_proto.Repo(name=repo_name))
        res = self._pfs_stub.ListBranch(req, metadata=self.metadata)
        return res.branch_info

    def delete_branch(self, repo_name, branch_name, force=None):
        &#34;&#34;&#34;
        Deletes a branch, but leaves the commits themselves intact. In other
        words, those commits can still be accessed via commit IDs and other
        branches they happen to be on.

        Params:

        * repo_name: A string specifying the repo name.
        * branch_name: A string specifying the name of the branch to delete.
        * force: A bool specifying whether to force the branch deletion.
        &#34;&#34;&#34;
        branch = pfs_proto.Branch(repo=pfs_proto.Repo(name=repo_name), name=branch_name)
        req = pfs_proto.DeleteBranchRequest(branch=branch, force=force)
        self._pfs_stub.DeleteBranch(req, metadata=self.metadata)

    def put_file_bytes(self, commit, path, value, delimiter=None,
                       target_file_datums=None, target_file_bytes=None, overwrite_index=None):
        &#34;&#34;&#34;
        Uploads a binary bytes array as file(s) in a certain path.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * path: A string specifying the path in the repo the file(s) will be
        written to.
        * value: The file contents as bytes, represented as a file-like
        object, bytestring, or iterator of bytestrings.
        * delimiter: Optional. causes data to be broken up into separate files
        with `path` as a prefix.
        * target_file_datums: An optional int. Specifies the target number of
        datums in each written file. It may be lower if data does not split
        evenly, but will never be higher, unless the value is 0.
        * target_file_bytes: An optional int. Specifies the target number of
        bytes in each written file, files may have more or fewer bytes than
        the target.
        * overwrite_index: An optional `OverwriteIndex` object. This is the
        object index where the write starts from.  All existing objects
        starting from the index are deleted.
        &#34;&#34;&#34;

        overwrite_index = pfs_proto.OverwriteIndex(index=overwrite_index) if overwrite_index is not None else None

        if hasattr(value, &#34;read&#34;):
            def wrap(value):
                for i in itertools.count():
                    chunk = value.read(BUFFER_SIZE)

                    if len(chunk) == 0:
                        return

                    if i == 0:
                        yield pfs_proto.PutFileRequest(
                            file=pfs_proto.File(commit=commit_from(commit), path=path),
                            value=chunk,
                            delimiter=delimiter,
                            target_file_datums=target_file_datums,
                            target_file_bytes=target_file_bytes,
                            overwrite_index=overwrite_index
                        )
                    else:
                        yield pfs_proto.PutFileRequest(value=chunk)
        elif isinstance(value, collections.Iterable) and not isinstance(value, (str, bytes)):
            def wrap(value):
                for i, chunk in enumerate(value):
                    if i == 0:
                        yield pfs_proto.PutFileRequest(
                            file=pfs_proto.File(commit=commit_from(commit), path=path),
                            value=chunk,
                            delimiter=delimiter,
                            target_file_datums=target_file_datums,
                            target_file_bytes=target_file_bytes,
                            overwrite_index=overwrite_index
                        )
                    else:
                        yield pfs_proto.PutFileRequest(value=chunk)
        else:
            def wrap(value):
                yield pfs_proto.PutFileRequest(
                    file=pfs_proto.File(commit=commit_from(commit), path=path),
                    value=value[:BUFFER_SIZE],
                    delimiter=delimiter,
                    target_file_datums=target_file_datums,
                    target_file_bytes=target_file_bytes,
                    overwrite_index=overwrite_index
                )

                for i in range(BUFFER_SIZE, len(value), BUFFER_SIZE):
                    yield pfs_proto.PutFileRequest(
                        value=value[i:i + BUFFER_SIZE],
                        overwrite_index=overwrite_index
                    )

        self._pfs_stub.PutFile(wrap(value), metadata=self.metadata)

    def put_file_url(self, commit, path, url, recursive=None, overwrite_index=None):
        &#34;&#34;&#34;
        Puts a file using the content found at a URL. The URL is sent to the
        server which performs the request. Note that this is not a standard
        PFS function.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * path: A string specifying the path to the file.
        * url: A string specifying the url of the file to put.
        * recursive: allow for recursive scraping of some types URLs, for
        example on s3:// URLs.
        * overwrite_index: An optional `OverwriteIndex` object. This is the
        object index where the write starts from.  All existing objects
        starting from the index are deleted.
        &#34;&#34;&#34;

        overwrite_index = pfs_proto.OverwriteIndex(index=overwrite_index) if overwrite_index is not None else None

        req = iter([
            pfs_proto.PutFileRequest(
                file=pfs_proto.File(commit=commit_from(commit), path=path),
                url=url,
                recursive=recursive,
                overwrite_index=overwrite_index
            )
        ])
        self._pfs_stub.PutFile(req, metadata=self.metadata)

    def copy_file(self, source_commit, source_path, dest_commit, dest_path, overwrite=None):
        &#34;&#34;&#34;
        Efficiently copies files already in PFS. Note that the destination
        repo cannot be an output repo, or the copy operation will (as of
        1.9.0) silently fail.

        Params:

        * source_commit: A tuple, string, or `Commit` object representing the
        commit for the source file.
        * source_path: A string specifying the path of the source file.
        * dest_commit: A tuple, string, or `Commit` object representing the
        commit for the destination file.
        * dest_path: A string specifying the path of the destination file.
        * overwrite: Am optional bool specifying whether to overwrite the
        destination file if it already exists.
        &#34;&#34;&#34;
        req = pfs_proto.CopyFileRequest(
            src=pfs_proto.File(commit=commit_from(source_commit), path=source_path),
            dst=pfs_proto.File(commit=commit_from(dest_commit), path=dest_path),
            overwrite=overwrite,
        )
        self._pfs_stub.CopyFile(req, metadata=self.metadata)

    def get_file(self, commit, path, offset_bytes=None, size_bytes=None):
        &#34;&#34;&#34;
        Returns an iterator of the contents of a file at a specific commit.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * path: A string specifying the path of the file.
        * offset_bytes: An optional int. Specifies a number of bytes that
        should be skipped in the beginning of the file.
        * size_bytes: An optional int. limits the total amount of data
        returned, note you will get fewer bytes than size if you pass a value
        larger than the size of the file. If size is set to 0 then all of the
        data will be returned.
        &#34;&#34;&#34;
        req = pfs_proto.GetFileRequest(
            file=pfs_proto.File(commit=commit_from(commit), path=path),
            offset_bytes=offset_bytes,
            size_bytes=size_bytes
        )
        res = self._pfs_stub.GetFile(req, metadata=self.metadata)
        for item in res:
            yield item.value

    def inspect_file(self, commit, path):
        &#34;&#34;&#34;
        Inspects a file. Returns a `FileInfo` object.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * path: A string specifying the path to the file.
        &#34;&#34;&#34;
        req = pfs_proto.InspectFileRequest(file=pfs_proto.File(commit=commit_from(commit), path=path))
        return self._pfs_stub.InspectFile(req, metadata=self.metadata)

    def list_file(self, commit, path, history=None, include_contents=None):
        &#34;&#34;&#34;
        Lists the files in a directory.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * path: The path to the directory.
        * history: An optional int that indicates to return jobs from
        historical versions of pipelines. Semantics are:
         0: Return jobs from the current version of the pipeline or pipelines.
         1: Return the above and jobs from the next most recent version
         2: etc.
        -1: Return jobs from all historical versions.
        * include_contents: An optional bool. If `True`, file contents are
        included.
        &#34;&#34;&#34;

        req = pfs_proto.ListFileRequest(
            file=pfs_proto.File(commit=commit_from(commit), path=path),
            history=history,
            full=include_contents,
        )

        return self._pfs_stub.ListFileStream(req, metadata=self.metadata)

    def walk_file(self, commit, path):
        &#34;&#34;&#34;
        Walks over all descendant files in a directory. Returns a generator of
        `FileInfo` objects.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * path: The path to the directory.
        &#34;&#34;&#34;
        commit = commit_from(commit)
        f = pfs_proto.File(commit=commit_from(commit), path=path)
        req = pfs_proto.WalkFileRequest(file=f)
        return self._pfs_stub.WalkFile(req, metadata=self.metadata)

    def glob_file(self, commit, pattern):
        &#34;&#34;&#34;
        Lists files that match a glob pattern. Yields `FileInfo` objects.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * pattern: A string representing a glob pattern.
        &#34;&#34;&#34;

        req = pfs_proto.GlobFileRequest(commit=commit_from(commit), pattern=pattern)
        return self._pfs_stub.GlobFileStream(req, metadata=self.metadata)

    def delete_file(self, commit, path):
        &#34;&#34;&#34;
        Deletes a file from a Commit. DeleteFile leaves a tombstone in the
        Commit, assuming the file isn&#39;t written to later attempting to get the
        file from the finished commit will result in not found error. The file
        will of course remain intact in the Commit&#39;s parent.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * path: The path to the file.
        &#34;&#34;&#34;
        req = pfs_proto.DeleteFileRequest(file=pfs_proto.File(commit=commit_from(commit), path=path))
        self._pfs_stub.DeleteFile(req, metadata=self.metadata)

    def inspect_job(self, job_id, block_state=None, output_commit=None):
        &#34;&#34;&#34;
        Inspects a job with a given ID. Returns a `JobInfo`.

        Params:

        * job_id: The ID of the job to inspect.
        * block_state: If true, block until the job completes.
        * output_commit: An optional tuple, string, or `Commit` object
        representing an output commit to filter on.
        &#34;&#34;&#34;

        output_commit = commit_from(output_commit) if output_commit is not None else None
        req = pps_proto.InspectJobRequest(job=pps_proto.Job(id=job_id), block_state=block_state, output_commit=output_commit)
        return self._pps_stub.InspectJob(req, metadata=self.metadata)

    def list_job(self, pipeline_name=None, input_commit=None, output_commit=None, history=None):
        &#34;&#34;&#34;
        Lists jobs. Yields `JobInfo` objects.

        Params:

        * pipeline_name: An optional string representing a pipeline name to
        filter on.
        * input_commit: An optional list of tuples, strings, or `Commit`
        objects representing input commits to filter on.
        * output_commit: An optional tuple, string, or `Commit` object
        representing an output commit to filter on.
        * history: An optional int that indicates to return jobs from
          historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        &#34;&#34;&#34;

        pipeline = pps_proto.Pipeline(name=pipeline_name) if pipeline_name is not None else None

        if isinstance(input_commit, list):
            input_commit = [commit_from(ic) for ic in input_commit]
        elif input_commit is not None:
            input_commit = [commit_from(input_commit)]

        output_commit = commit_from(output_commit) if output_commit is not None else None

        req = pps_proto.ListJobRequest(pipeline=pipeline, input_commit=input_commit,
                                   output_commit=output_commit, history=history)

        return self._pps_stub.ListJobStream(req, metadata=self.metadata)

    def flush_job(self, commits, pipeline_names=None):
        &#34;&#34;&#34;
        Blocks until all of the jobs which have a set of commits as
        provenance have finished. Yields `JobInfo` objects.

        Params:

        * commits: A list of tuples, strings, or `Commit` objects representing
        the commits to flush.
        * pipeline_names: An optional list of strings specifying pipeline
        names. If specified, only jobs within these pipelines will be flushed.
        &#34;&#34;&#34;

        commits = [commit_from(c) for c in commits]
        pipelines = [pps_proto.Pipeline(name=name) for name in pipeline_names] if pipeline_names is not None else None
        req = pps_proto.FlushJobRequest(commits=commits, to_pipelines=pipelines)
        return self._pps_stub.FlushJob(req)

    def delete_job(self, job_id):
        &#34;&#34;&#34;
        Deletes a job by its ID.

        Params:

        * job_id: The ID of the job to delete.
        &#34;&#34;&#34;

        req = pps_proto.DeleteJobRequest(job=pps_proto.Job(id=job_id))
        self._pps_stub.DeleteJob(req, metadata=self.metadata)

    def stop_job(self, job_id):
        &#34;&#34;&#34;
        Stops a job by its ID.

        Params:

        * job_id: The ID of the job to stop.
        &#34;&#34;&#34;

        req = pps_proto.StopJobRequest(job=pps_proto.Job(id=job_id))
        self._pps_stub.StopJob(req, metadata=self.metadata)

    def inspect_datum(self, job_id, datum_id):
        &#34;&#34;&#34;
        Inspects a datum. Returns a `DatumInfo` object.

        Params:

        * job_id: The ID of the job.
        * datum_id: The ID of the datum.
        &#34;&#34;&#34;

        req = pps_proto.InspectDatumRequest(datum=pps_proto.Datum(id=datum_id, job=pps_proto.Job(id=job_id)))
        return self._pps_stub.InspectDatum(req, metadata=self.metadata)

    def list_datum(self, job_id, page_size=None, page=None):
        &#34;&#34;&#34;
        Lists datums. Yields `ListDatumStreamResponse` objects.

        Params:

        * job_id: The ID of the job.
        * page_size: An optional int specifying the size of the page.
        * page: An optional int specifying the page number.
        &#34;&#34;&#34;

        req = pps_proto.ListDatumRequest(job=pps_proto.Job(id=job_id), page_size=page_size, page=page)
        return self._pps_stub.ListDatumStream(req, metadata=self.metadata)

    def restart_datum(self, job_id, data_filters=None):
        &#34;&#34;&#34;
        Restarts a datum.

        Params:

        * job_id: The ID of the job.
        * data_filters: An optional iterable of strings.
        &#34;&#34;&#34;

        req = pps_proto.RestartDatumRequest(job=pps_proto.Job(id=job_id), data_filters=data_filters)
        self._pps_stub.RestartDatum(req, metadata=self.metadata)

    def create_pipeline(self, pipeline_name, transform=None, parallelism_spec=None,
                        hashtree_spec=None, egress=None, update=None, output_branch=None,
                        scale_down_threshold=None, resource_requests=None,
                        resource_limits=None, input=None, description=None, cache_size=None,
                        enable_stats=None, reprocess=None, batch=None, max_queue_size=None,
                        service=None, chunk_spec=None, datum_timeout=None,
                        job_timeout=None, salt=None, standby=None, datum_tries=None,
                        scheduling_spec=None, pod_patch=None):
        &#34;&#34;&#34;
        Creates a pipeline. For more info, please refer to the pipeline spec
        document:
        http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html

        Params:

        * pipeline_name: A string representing the pipeline name.
        * transform: An optional `Transform` object.
        * parallelism_spec: An optional `ParallelismSpec` object.
        * hashtree_spec: An optional `HashtreeSpec` object.
        * egress: An optional `Egress` object.
        * update: An optional bool specifying whether this should behave as an
        upsert.
        * output_branch: An optional string representing the branch to output
        results on.
        * scale_down_threshold: An optional pps_proto.uf `Duration` object.
        * resource_requests: An optional `ResourceSpec` object.
        * resource_limits: An optional `ResourceSpec` object.
        * input: An optional `Input` object.
        * description: An optional string describing the pipeline.
        * cache_size: An optional string.
        * enable_stats: An optional bool.
        * reprocess: An optional bool. If true, pachyderm forces the pipeline
        to reprocess all datums. It only has meaning if `update` is `True`.
        * batch: An optional bool.
        * max_queue_size: An optional int.
        * service: An optional `Service` object.
        * chunk_spec: An optional `ChunkSpec` object.
        * datum_timeout: An optional pps_proto.uf `Duration` object.
        * job_timeout: An optional pps_proto.uf `Duration` object.
        * salt: An optional stirng.
        * standby: An optional bool.
        * datum_tries: An optional int.
        * scheduling_spec: An optional `SchedulingSpec` object.
        * pod_patch: An optional string.
        &#34;&#34;&#34;

        req = pps_proto.CreatePipelineRequest(
            pipeline=pps_proto.Pipeline(name=pipeline_name),
            transform=transform, parallelism_spec=parallelism_spec,
            hashtree_spec=hashtree_spec, egress=egress, update=update,
            output_branch=output_branch, scale_down_threshold=scale_down_threshold,
            resource_requests=resource_requests, resource_limits=resource_limits,
            input=input, description=description, cache_size=cache_size,
            enable_stats=enable_stats, reprocess=reprocess, batch=batch,
            max_queue_size=max_queue_size, service=service,
            chunk_spec=chunk_spec, datum_timeout=datum_timeout,
            job_timeout=job_timeout, salt=salt, standby=standby,
            datum_tries=datum_tries, scheduling_spec=scheduling_spec,
            pod_patch=pod_patch
        )
        self._pps_stub.CreatePipeline(req, metadata=self.metadata)

    def inspect_pipeline(self, pipeline_name, history=None):
        &#34;&#34;&#34;
        Inspects a pipeline. Returns a `PipelineInfo` object.

        Params:

        * pipeline_name: A string representing the pipeline name.
        * history: An optional int that indicates to return jobs from
        historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        &#34;&#34;&#34;

        pipeline = pps_proto.Pipeline(name=pipeline_name)

        if history is None:
            req = pps_proto.InspectPipelineRequest(pipeline=pipeline)
            return self._pps_stub.InspectPipeline(req, metadata=self.metadata)
        else:
            # `InspectPipeline` doesn&#39;t support history, but `ListPipeline`
            # with a pipeline filter does, so we use that here
            req = pps_proto.ListPipelineRequest(pipeline=pipeline, history=history)
            pipelines = self._pps_stub.ListPipeline(req, metadata=self.metadata).pipeline_info
            assert len(pipelines) &lt;= 1
            return pipelines[0] if len(pipelines) else None

    def list_pipeline(self, history=None):
        &#34;&#34;&#34;
        Lists pipelines. Returns a `PipelineInfos` object.

        Params:

        * pipeline_name: A string representing the pipeline name.
        * history: An optional int that indicates to return jobs from
        historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        &#34;&#34;&#34;
        req = pps_proto.ListPipelineRequest(history=history)
        return self._pps_stub.ListPipeline(req, metadata=self.metadata)

    def delete_pipeline(self, pipeline_name, force=None):
        &#34;&#34;&#34;
        Deletes a pipeline.

        Params:

        * pipeline_name: A string representing the pipeline name.
        * force: Whether to force delete.
        &#34;&#34;&#34;

        req = pps_proto.DeletePipelineRequest(pipeline=pps_proto.Pipeline(name=pipeline_name), force=force)
        self._pps_stub.DeletePipeline(req, metadata=self.metadata)

    def delete_all_pipelines(self, force=None):
        &#34;&#34;&#34;
        Deletes all pipelines.

        Params:

        * force: Whether to force delete.
        &#34;&#34;&#34;

        req = pps_proto.DeletePipelineRequest(all=True, force=force)
        self._pps_stub.DeletePipeline(req, metadata=self.metadata)

    def start_pipeline(self, pipeline_name):
        &#34;&#34;&#34;
        Starts a pipeline.

        Params:

        * pipeline_name: A string representing the pipeline name.
        &#34;&#34;&#34;

        req = pps_proto.StartPipelineRequest(pipeline=pps_proto.Pipeline(name=pipeline_name))
        self._pps_stub.StartPipeline(req, metadata=self.metadata)

    def stop_pipeline(self, pipeline_name):
        &#34;&#34;&#34;
        Stops a pipeline.

        Params:

        * pipeline_name: A string representing the pipeline name.
        &#34;&#34;&#34;
        req = pps_proto.StopPipelineRequest(pipeline=pps_proto.Pipeline(name=pipeline_name))
        self._pps_stub.StopPipeline(req, metadata=self.metadata)

    def run_pipeline(self, pipeline_name, provenance=None):
        &#34;&#34;&#34;
        Runs a pipeline.

        Params:

        * pipeline_name: A string representing the pipeline name.
        * provenance: An optional iterable of `CommitProvenance` objects
        representing the pipeline execution provenance.
        &#34;&#34;&#34;
        req = pps_proto.RunPipelineRequest(
            pipeline=pps_proto.Pipeline(name=pipeline_name),
            provenance=provenance,
        )
        self._pps_stub.RunPipeline(req, metadata=self.metadata)

    def delete_all(self):
        &#34;&#34;&#34;
        Deletes everything in pachyderm.
        &#34;&#34;&#34;
        req = pps_proto.google_dot_pps_proto.uf_dot_empty__pb2.Empty()
        self._pps_stub.DeleteAll(req, metadata=self.metadata)

    def get_pipeline_logs(self, pipeline_name, data_filters=None, master=None,
                          datum=None, follow=None, tail=None):
        &#34;&#34;&#34;
        Gets logs for a pipeline. Yields `LogMessage` objects.

        Params:

        * pipeline_name: A string representing a pipeline to get
        logs of.
        * data_filters: An optional iterable of strings specifying the names
        of input files from which we want processing logs. This may contain
        multiple files, to query pipelines that contain multiple inputs. Each
        filter may be an absolute path of a file within a pps repo, or it may
        be a hash for that file (to search for files at specific versions.)
        * master: An optional bool.
        * datum: An optional `Datum` object.
        * follow: An optional bool specifying whether logs should continue to
        stream forever.
        * tail: An optional int. If nonzero, the number of lines from the end
        of the logs to return.  Note: tail applies per container, so you will
        get tail * &lt;number of pods&gt; total lines back.
        &#34;&#34;&#34;

        req = pps_proto.GetLogsRequest(
            pipeline=pps_proto.Pipeline(name=pipeline_name),
            data_filters=data_filters, master=master, datum=datum,
            follow=follow, tail=tail,
        )
        return self._pps_stub.GetLogs(req, metadata=self.metadata)

    def get_job_logs(self, job_id, data_filters=None, datum=None, follow=None,
                     tail=None):
        &#34;&#34;&#34;
        Gets logs for a job. Yields `LogMessage` objects.

        Params:

        * job_id: A string representing a job to get logs of.
        * data_filters: An optional iterable of strings specifying the names
        of input files from which we want processing logs. This may contain
        multiple files, to query pipelines that contain multiple inputs. Each
        filter may be an absolute path of a file within a pps repo, or it may
        be a hash for that file (to search for files at specific versions.)
        * datum: An optional `Datum` object.
        * follow: An optional bool specifying whether logs should continue to
        stream forever.
        * tail: An optional int. If nonzero, the number of lines from the end
        of the logs to return.  Note: tail applies per container, so you will
        get tail * &lt;number of pods&gt; total lines back.
        &#34;&#34;&#34;

        req = pps_proto.GetLogsRequest(
            job=pps_proto.Job(id=job_id), data_filters=data_filters, datum=datum,
            follow=follow, tail=tail,
        )
        return self._pps_stub.GetLogs(req, metadata=self.metadata)

    def garbage_collect(self):
        &#34;&#34;&#34;
        Runs garbage collection.
        &#34;&#34;&#34;
        return self._pps_stub.GarbageCollect(pps_proto.GarbageCollectRequest(), metadata=self.metadata)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="python_pachyderm.client.commit_from"><code class="name flex">
<span>def <span class="ident">commit_from</span></span>(<span>src, allow_just_repo=False)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def commit_from(src, allow_just_repo=False):
    if isinstance(src, pfs_proto.Commit):
        return src
    elif isinstance(src, (tuple, list)) and len(src) == 2:
        return pfs_proto.Commit(repo=pfs_proto.Repo(name=src[0]), id=src[1])
    elif isinstance(src, str):
        repo_name, commit_id = src.split(&#39;/&#39;, 1)
        return pfs_proto.Commit(repo=pfs_proto.Repo(name=repo_name), id=commit_id)

    if not allow_just_repo:
        raise ValueError(&#34;Invalid commit type&#34;)
    return pfs_proto.Commit(repo=pfs_proto.Repo(name=src))</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="python_pachyderm.client.Client"><code class="flex name class">
<span>class <span class="ident">Client</span></span>
<span>(</span><span>host=None, port=None, auth_token=None, root_certs=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Creates a client to connect to PFS.</p>
<p>Params:</p>
<ul>
<li>host: The pachd host. Default is 'localhost', which is used with
<code>pachctl port-forward</code>.</li>
<li>port: The port to connect to. Default is 30650.</li>
<li>auth_token: The authentication token; used if authentication is
enabled on the cluster. Default to <code>None</code>.</li>
<li>root_certs:
The PEM-encoded root certificates as byte string.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class Client(object):
    def __init__(self, host=None, port=None, auth_token=None, root_certs=None):
        &#34;&#34;&#34;
        Creates a client to connect to PFS.

        Params:

        * host: The pachd host. Default is &#39;localhost&#39;, which is used with
        `pachctl port-forward`.
        * port: The port to connect to. Default is 30650.
        * auth_token: The authentication token; used if authentication is
        enabled on the cluster. Default to `None`.
        * root_certs:  The PEM-encoded root certificates as byte string.
        &#34;&#34;&#34;

        if host is not None and port is not None:
            self.address = &#34;{}:{}&#34;.format(host, port)
        else:
            self.address = os.environ.get(&#34;PACHD_ADDRESS&#34;, &#34;localhost:30650&#34;)

        if auth_token is None:
            auth_token = os.environ.get(&#34;PACH_PYTHON_AUTH_TOKEN&#34;)

        self.metadata = []
        if auth_token is not None:
            self.metadata.append((&#34;authn-token&#34;, auth_token))

        self.root_certs = root_certs

    @property
    def _pfs_stub(self):
        if not hasattr(self, &#34;__pfs_stub&#34;):
            if self.root_certs:
                ssl_channel_credentials = pfs_grpc.grpc.ssl_channel_credentials
                ssl = ssl_channel_credentials(root_certificates=root_certs)
                channel = pfs_grpc.grpc.secure_channel(self.address, ssl)
            else:
                channel = pfs_grpc.grpc.insecure_channel(self.address)
            self.__pfs_stub = pfs_grpc.APIStub(channel)
        return self.__pfs_stub

    @property
    def _pps_stub(self):
        if not hasattr(self, &#34;__pps_stub&#34;):
            if self.root_certs:
                ssl_channel_credentials = pps_grpc.grpc.ssl_channel_credentials
                ssl = ssl_channel_credentials(root_certificates=root_certs)
                channel = pps_grpc.grpc.secure_channel(self.address, ssl)
            else:
                channel = pps_grpc.grpc.insecure_channel(self.address)
            self.__pps_stub = pps_grpc.APIStub(channel)
        return self.__pps_stub

    def get_remote_version(self):
        with closing(version_grpc.grpc.insecure_channel(self.address)) as channel:
            stub = version_grpc.APIStub(channel)
            return stub.GetVersion(version_grpc.google_dot_protobuf_dot_empty__pb2.Empty())

    def create_repo(self, repo_name, description=None, update=None):
        &#34;&#34;&#34;
        Creates a new `Repo` object in PFS with the given name. Repos are the
        top level data object in PFS and should be used to store data of a
        similar type. For example rather than having a single `Repo` for an
        entire project you might have separate `Repo`s for logs, metrics,
        database dumps etc.

        Params:

        * repo_name: Name of the repo.
        * description: An optional string describing the repo.
        * update: Whether to update if the repo already exists.
        &#34;&#34;&#34;
        req = pfs_proto.CreateRepoRequest(
            repo=pfs_proto.Repo(name=repo_name),
            description=description,
            update=update
        )
        self._pfs_stub.CreateRepo(req, metadata=self.metadata)

    def inspect_repo(self, repo_name):
        &#34;&#34;&#34;
        Returns info about a specific repo. Returns a `RepoInfo` object.

        Params:
        * repo_name: Name of the repo.
        &#34;&#34;&#34;
        req = pfs_proto.InspectRepoRequest(repo=pfs_proto.Repo(name=repo_name))
        return self._pfs_stub.InspectRepo(req, metadata=self.metadata)

    def list_repo(self):
        &#34;&#34;&#34;
        Returns info about all repos, as a list of `RepoInfo` objects.
        &#34;&#34;&#34;
        req = pfs_proto.ListRepoRequest()
        res = self._pfs_stub.ListRepo(req, metadata=self.metadata)
        return res.repo_info

    def delete_repo(self, repo_name, force=None):
        &#34;&#34;&#34;
        Deletes a repo and reclaims the storage space it was using.

        Params:

        * repo_name: The name of the repo.
        * force: If set to true, the repo will be removed regardless of
        errors. This argument should be used with care.
        &#34;&#34;&#34;
        req = pfs_proto.DeleteRepoRequest(repo=pfs_proto.Repo(name=repo_name), force=force, all=False)
        self._pfs_stub.DeleteRepo(req, metadata=self.metadata)

    def delete_all_repos(self, force=None):
        &#34;&#34;&#34;
        Deletes all repos.

        Params:

        * force: If set to true, the repo will be removed regardless of
        errors. This argument should be used with care.
        &#34;&#34;&#34;

        req = pfs_proto.DeleteRepoRequest(force=force, all=True)
        self._pfs_stub.DeleteRepo(req, metadata=self.metadata)

    def start_commit(self, repo_name, branch=None, parent=None, description=None, provenance=None):
        &#34;&#34;&#34;
        Begins the process of committing data to a Repo. Once started you can
        write to the Commit with PutFile and when all the data has been
        written you must finish the Commit with FinishCommit. NOTE, data is
        not persisted until FinishCommit is called. A Commit object is
        returned.

        Params:

        * repo_name: A string specifying the name of the repo.
        * branch: A string specifying the branch name. This is a more
        convenient way to build linear chains of commits. When a commit is
        started with a non-empty branch the value of branch becomes an alias
        for the created Commit. This enables a more intuitive access pattern.
        When the commit is started on a branch the previous head of the branch
        is used as the parent of the commit.
        * parent: An optional `Commit` object specifying the parent commit.
        Upon creation the new commit will appear identical to the parent
        commit, data can safely be added to the new commit without affecting
        the contents of the parent commit.
        * description: An optional string describing the commit.
        * provenance: An optional iterable of `CommitProvenance` objects
        specifying the commit provenance.
        &#34;&#34;&#34;
        req = pfs_proto.StartCommitRequest(
            parent=pfs_proto.Commit(repo=pfs_proto.Repo(name=repo_name), id=parent),
            branch=branch,
            description=description,
            provenance=provenance,
        )
        return self._pfs_stub.StartCommit(req, metadata=self.metadata)

    def finish_commit(self, commit, description=None,
                      tree_object_hashes=None, datum_object_hash=None,
                      size_bytes=None, empty=None):
        &#34;&#34;&#34;
        Ends the process of committing data to a Repo and persists the
        Commit. Once a Commit is finished the data becomes immutable and
        future attempts to write to it with PutFile will error.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * description: An optional string describing this commit.
        * tree_object_hashes: A list of zero or more strings specifying object
        hashes.
        * datum_object_hash: An optional string specifying an object hash.
        * size_bytes: An optional int.
        * empty: An optional bool. If set, the commit will be closed (its
        `finished` field will be set to the current time) but its `tree` will
        be left nil.
        &#34;&#34;&#34;
        req = pfs_proto.FinishCommitRequest(
            commit=commit_from(commit),
            description=description,
            trees=[pfs_proto.Object(hash=h) for h in tree_object_hashes] if tree_object_hashes is not None else None,
            datums=pfs_proto.Object(hash=datum_object_hash) if datum_object_hash is not None else None,
            size_bytes=size_bytes,
            empty=empty,
        )
        return self._pfs_stub.FinishCommit(req, metadata=self.metadata)

    @contextmanager
    def commit(self, repo_name, branch=None, parent=None, description=None):
        &#34;&#34;&#34;
        A context manager for running operations within a commit.

        Params:

        * repo_name: A string specifying the name of the repo.
        * branch: A string specifying the branch name. This is a more
        convenient way to build linear chains of commits. When a commit is
        started with a non-empty branch the value of branch becomes an alias
        for the created Commit. This enables a more intuitive access pattern.
        When the commit is started on a branch the previous head of the branch
        is used as the parent of the commit.
        * parent: An optional `Commit` object specifying the parent commit.
        Upon creation the new commit will appear identical to the parent
        commit, data can safely be added to the new commit without affecting
        the contents of the parent commit.
        * description: An optional string describing the commit.
        &#34;&#34;&#34;
        commit = self.start_commit(repo_name, branch, parent, description)
        try:
            yield commit
        finally:
            self.finish_commit(commit)

    def inspect_commit(self, commit, block_state=None):
        &#34;&#34;&#34;
        Inspects a commit. Returns a `CommitInfo` object.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * block_state: Causes inspect commit to block until the commit is in
        the desired commit state.
        &#34;&#34;&#34;
        req = pfs_proto.InspectCommitRequest(commit=commit_from(commit), block_state=block_state)
        return self._pfs_stub.InspectCommit(req, metadata=self.metadata)

    def list_commit(self, repo_name, to_commit=None, from_commit=None, number=None):
        &#34;&#34;&#34;
        Lists commits. Yields `CommitInfo` objects.

        Params:

        * repo_name: If only `repo_name` is given, all commits in the repo are
        returned.
        * to_commit: Optional. Only the ancestors of `to`, including `to`
        itself, are considered.
        * from_commit: Optional. Only the descendants of `from`, including
        `from` itself, are considered.
        * number: Optional. Determines how many commits are returned.  If
        `number` is 0, all commits that match the aforementioned criteria are
        returned.
        &#34;&#34;&#34;
        req = pfs_proto.ListCommitRequest(repo=pfs_proto.Repo(name=repo_name), number=number)
        if to_commit is not None:
            req.to.CopyFrom(commit_from(to_commit))
        if from_commit is not None:
            getattr(req, &#39;from&#39;).CopyFrom(commit_from(from_commit))
        return self._pfs_stub.ListCommitStream(req, metadata=self.metadata)

    def delete_commit(self, commit):
        &#34;&#34;&#34;
        Deletes a commit.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        &#34;&#34;&#34;
        req = pfs_proto.DeleteCommitRequest(commit=commit_from(commit))
        self._pfs_stub.DeleteCommit(req, metadata=self.metadata)

    def flush_commit(self, commits, repos=None):
        &#34;&#34;&#34;
        Blocks until all of the commits which have a set of commits as
        provenance have finished. For commits to be considered they must have
        all of the specified commits as provenance. This in effect waits for
        all of the jobs that are triggered by a set of commits to complete.
        It returns an error if any of the commits it&#39;s waiting on are
        cancelled due to one of the jobs encountering an error during runtime.
        Note that it&#39;s never necessary to call FlushCommit to run jobs,
        they&#39;ll run no matter what, FlushCommit just allows you to wait for
        them to complete and see their output once they do. This returns an
        iterator of CommitInfo objects.

        Yields `CommitInfo` objects.

        Params:

        * commits: A list of tuples, strings, or `Commit` objects representing
        the commits to flush.
        * repos: An optional list of strings specifying repo names. If
        specified, only commits within these repos will be flushed.
        &#34;&#34;&#34;
        to_repos = [pfs_proto.Repo(name=r) for r in repos] if repos is not None else None
        req = pfs_proto.FlushCommitRequest(commits=[commit_from(c) for c in commits],
                                       to_repos=to_repos)
        return self._pfs_stub.FlushCommit(req, metadata=self.metadata)

    def subscribe_commit(self, repo_name, branch, from_commit_id=None, state=None):
        &#34;&#34;&#34;
        Yields `CommitInfo` objects as commits occur.

        Params:

        * repo_name: A string specifying the name of the repo.
        * branch: A string specifying branch to subscribe to.
        * from_commit_id: An optional string specifying the commit ID. Only
        commits created since this commit are returned.
        * state: The commit state to filter on.
        &#34;&#34;&#34;
        repo = pfs_proto.Repo(name=repo_name)
        req = pfs_proto.SubscribeCommitRequest(repo=repo, branch=branch, state=state)
        if from_commit_id is not None:
            getattr(req, &#39;from&#39;).CopyFrom(pfs_proto.Commit(repo=repo, id=from_commit_id))
        return self._pfs_stub.SubscribeCommit(req, metadata=self.metadata)

    def create_branch(self, repo_name, branch_name, commit=None, provenance=None):
        &#34;&#34;&#34;
        Creates a new branch.

        Params:
        * repo_name: A string specifying the name of the repo.
        * branch_name: A string specifying the new branch name.
        * commit: An optional tuple, string, or `Commit` object representing
        the head commit of the branch.
        * provenance: An optional iterable of `Branch` objects representing
        the branch provenance.
        &#34;&#34;&#34;
        req = pfs_proto.CreateBranchRequest(
            branch=pfs_proto.Branch(repo=pfs_proto.Repo(name=repo_name), name=branch_name),
            head=commit_from(commit) if commit is not None else None,
            provenance=provenance,
        )
        self._pfs_stub.CreateBranch(req, metadata=self.metadata)

    def inspect_branch(self, repo_name, branch_name):
        &#34;&#34;&#34;
        Inspects a branch. Returns a `BranchInfo` object.
        &#34;&#34;&#34;
        branch = pfs_proto.Branch(repo=pfs_proto.Repo(name=repo_name), name=branch_name)
        req = pfs_proto.InspectBranchRequest(branch=branch)
        return self._pfs_stub.InspectBranch(req, metadata=self.metadata)

    def list_branch(self, repo_name):
        &#34;&#34;&#34;
        Lists the active branch objects on a repo. Returns a list of
        `BranchInfo` objects.

        Params:

        * repo_name: A string specifying the repo name.
        &#34;&#34;&#34;
        req = pfs_proto.ListBranchRequest(repo=pfs_proto.Repo(name=repo_name))
        res = self._pfs_stub.ListBranch(req, metadata=self.metadata)
        return res.branch_info

    def delete_branch(self, repo_name, branch_name, force=None):
        &#34;&#34;&#34;
        Deletes a branch, but leaves the commits themselves intact. In other
        words, those commits can still be accessed via commit IDs and other
        branches they happen to be on.

        Params:

        * repo_name: A string specifying the repo name.
        * branch_name: A string specifying the name of the branch to delete.
        * force: A bool specifying whether to force the branch deletion.
        &#34;&#34;&#34;
        branch = pfs_proto.Branch(repo=pfs_proto.Repo(name=repo_name), name=branch_name)
        req = pfs_proto.DeleteBranchRequest(branch=branch, force=force)
        self._pfs_stub.DeleteBranch(req, metadata=self.metadata)

    def put_file_bytes(self, commit, path, value, delimiter=None,
                       target_file_datums=None, target_file_bytes=None, overwrite_index=None):
        &#34;&#34;&#34;
        Uploads a binary bytes array as file(s) in a certain path.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * path: A string specifying the path in the repo the file(s) will be
        written to.
        * value: The file contents as bytes, represented as a file-like
        object, bytestring, or iterator of bytestrings.
        * delimiter: Optional. causes data to be broken up into separate files
        with `path` as a prefix.
        * target_file_datums: An optional int. Specifies the target number of
        datums in each written file. It may be lower if data does not split
        evenly, but will never be higher, unless the value is 0.
        * target_file_bytes: An optional int. Specifies the target number of
        bytes in each written file, files may have more or fewer bytes than
        the target.
        * overwrite_index: An optional `OverwriteIndex` object. This is the
        object index where the write starts from.  All existing objects
        starting from the index are deleted.
        &#34;&#34;&#34;

        overwrite_index = pfs_proto.OverwriteIndex(index=overwrite_index) if overwrite_index is not None else None

        if hasattr(value, &#34;read&#34;):
            def wrap(value):
                for i in itertools.count():
                    chunk = value.read(BUFFER_SIZE)

                    if len(chunk) == 0:
                        return

                    if i == 0:
                        yield pfs_proto.PutFileRequest(
                            file=pfs_proto.File(commit=commit_from(commit), path=path),
                            value=chunk,
                            delimiter=delimiter,
                            target_file_datums=target_file_datums,
                            target_file_bytes=target_file_bytes,
                            overwrite_index=overwrite_index
                        )
                    else:
                        yield pfs_proto.PutFileRequest(value=chunk)
        elif isinstance(value, collections.Iterable) and not isinstance(value, (str, bytes)):
            def wrap(value):
                for i, chunk in enumerate(value):
                    if i == 0:
                        yield pfs_proto.PutFileRequest(
                            file=pfs_proto.File(commit=commit_from(commit), path=path),
                            value=chunk,
                            delimiter=delimiter,
                            target_file_datums=target_file_datums,
                            target_file_bytes=target_file_bytes,
                            overwrite_index=overwrite_index
                        )
                    else:
                        yield pfs_proto.PutFileRequest(value=chunk)
        else:
            def wrap(value):
                yield pfs_proto.PutFileRequest(
                    file=pfs_proto.File(commit=commit_from(commit), path=path),
                    value=value[:BUFFER_SIZE],
                    delimiter=delimiter,
                    target_file_datums=target_file_datums,
                    target_file_bytes=target_file_bytes,
                    overwrite_index=overwrite_index
                )

                for i in range(BUFFER_SIZE, len(value), BUFFER_SIZE):
                    yield pfs_proto.PutFileRequest(
                        value=value[i:i + BUFFER_SIZE],
                        overwrite_index=overwrite_index
                    )

        self._pfs_stub.PutFile(wrap(value), metadata=self.metadata)

    def put_file_url(self, commit, path, url, recursive=None, overwrite_index=None):
        &#34;&#34;&#34;
        Puts a file using the content found at a URL. The URL is sent to the
        server which performs the request. Note that this is not a standard
        PFS function.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * path: A string specifying the path to the file.
        * url: A string specifying the url of the file to put.
        * recursive: allow for recursive scraping of some types URLs, for
        example on s3:// URLs.
        * overwrite_index: An optional `OverwriteIndex` object. This is the
        object index where the write starts from.  All existing objects
        starting from the index are deleted.
        &#34;&#34;&#34;

        overwrite_index = pfs_proto.OverwriteIndex(index=overwrite_index) if overwrite_index is not None else None

        req = iter([
            pfs_proto.PutFileRequest(
                file=pfs_proto.File(commit=commit_from(commit), path=path),
                url=url,
                recursive=recursive,
                overwrite_index=overwrite_index
            )
        ])
        self._pfs_stub.PutFile(req, metadata=self.metadata)

    def copy_file(self, source_commit, source_path, dest_commit, dest_path, overwrite=None):
        &#34;&#34;&#34;
        Efficiently copies files already in PFS. Note that the destination
        repo cannot be an output repo, or the copy operation will (as of
        1.9.0) silently fail.

        Params:

        * source_commit: A tuple, string, or `Commit` object representing the
        commit for the source file.
        * source_path: A string specifying the path of the source file.
        * dest_commit: A tuple, string, or `Commit` object representing the
        commit for the destination file.
        * dest_path: A string specifying the path of the destination file.
        * overwrite: Am optional bool specifying whether to overwrite the
        destination file if it already exists.
        &#34;&#34;&#34;
        req = pfs_proto.CopyFileRequest(
            src=pfs_proto.File(commit=commit_from(source_commit), path=source_path),
            dst=pfs_proto.File(commit=commit_from(dest_commit), path=dest_path),
            overwrite=overwrite,
        )
        self._pfs_stub.CopyFile(req, metadata=self.metadata)

    def get_file(self, commit, path, offset_bytes=None, size_bytes=None):
        &#34;&#34;&#34;
        Returns an iterator of the contents of a file at a specific commit.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * path: A string specifying the path of the file.
        * offset_bytes: An optional int. Specifies a number of bytes that
        should be skipped in the beginning of the file.
        * size_bytes: An optional int. limits the total amount of data
        returned, note you will get fewer bytes than size if you pass a value
        larger than the size of the file. If size is set to 0 then all of the
        data will be returned.
        &#34;&#34;&#34;
        req = pfs_proto.GetFileRequest(
            file=pfs_proto.File(commit=commit_from(commit), path=path),
            offset_bytes=offset_bytes,
            size_bytes=size_bytes
        )
        res = self._pfs_stub.GetFile(req, metadata=self.metadata)
        for item in res:
            yield item.value

    def inspect_file(self, commit, path):
        &#34;&#34;&#34;
        Inspects a file. Returns a `FileInfo` object.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * path: A string specifying the path to the file.
        &#34;&#34;&#34;
        req = pfs_proto.InspectFileRequest(file=pfs_proto.File(commit=commit_from(commit), path=path))
        return self._pfs_stub.InspectFile(req, metadata=self.metadata)

    def list_file(self, commit, path, history=None, include_contents=None):
        &#34;&#34;&#34;
        Lists the files in a directory.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * path: The path to the directory.
        * history: An optional int that indicates to return jobs from
        historical versions of pipelines. Semantics are:
         0: Return jobs from the current version of the pipeline or pipelines.
         1: Return the above and jobs from the next most recent version
         2: etc.
        -1: Return jobs from all historical versions.
        * include_contents: An optional bool. If `True`, file contents are
        included.
        &#34;&#34;&#34;

        req = pfs_proto.ListFileRequest(
            file=pfs_proto.File(commit=commit_from(commit), path=path),
            history=history,
            full=include_contents,
        )

        return self._pfs_stub.ListFileStream(req, metadata=self.metadata)

    def walk_file(self, commit, path):
        &#34;&#34;&#34;
        Walks over all descendant files in a directory. Returns a generator of
        `FileInfo` objects.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * path: The path to the directory.
        &#34;&#34;&#34;
        commit = commit_from(commit)
        f = pfs_proto.File(commit=commit_from(commit), path=path)
        req = pfs_proto.WalkFileRequest(file=f)
        return self._pfs_stub.WalkFile(req, metadata=self.metadata)

    def glob_file(self, commit, pattern):
        &#34;&#34;&#34;
        Lists files that match a glob pattern. Yields `FileInfo` objects.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * pattern: A string representing a glob pattern.
        &#34;&#34;&#34;

        req = pfs_proto.GlobFileRequest(commit=commit_from(commit), pattern=pattern)
        return self._pfs_stub.GlobFileStream(req, metadata=self.metadata)

    def delete_file(self, commit, path):
        &#34;&#34;&#34;
        Deletes a file from a Commit. DeleteFile leaves a tombstone in the
        Commit, assuming the file isn&#39;t written to later attempting to get the
        file from the finished commit will result in not found error. The file
        will of course remain intact in the Commit&#39;s parent.

        Params:

        * commit: A tuple, string, or `Commit` object representing the commit.
        * path: The path to the file.
        &#34;&#34;&#34;
        req = pfs_proto.DeleteFileRequest(file=pfs_proto.File(commit=commit_from(commit), path=path))
        self._pfs_stub.DeleteFile(req, metadata=self.metadata)

    def inspect_job(self, job_id, block_state=None, output_commit=None):
        &#34;&#34;&#34;
        Inspects a job with a given ID. Returns a `JobInfo`.

        Params:

        * job_id: The ID of the job to inspect.
        * block_state: If true, block until the job completes.
        * output_commit: An optional tuple, string, or `Commit` object
        representing an output commit to filter on.
        &#34;&#34;&#34;

        output_commit = commit_from(output_commit) if output_commit is not None else None
        req = pps_proto.InspectJobRequest(job=pps_proto.Job(id=job_id), block_state=block_state, output_commit=output_commit)
        return self._pps_stub.InspectJob(req, metadata=self.metadata)

    def list_job(self, pipeline_name=None, input_commit=None, output_commit=None, history=None):
        &#34;&#34;&#34;
        Lists jobs. Yields `JobInfo` objects.

        Params:

        * pipeline_name: An optional string representing a pipeline name to
        filter on.
        * input_commit: An optional list of tuples, strings, or `Commit`
        objects representing input commits to filter on.
        * output_commit: An optional tuple, string, or `Commit` object
        representing an output commit to filter on.
        * history: An optional int that indicates to return jobs from
          historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        &#34;&#34;&#34;

        pipeline = pps_proto.Pipeline(name=pipeline_name) if pipeline_name is not None else None

        if isinstance(input_commit, list):
            input_commit = [commit_from(ic) for ic in input_commit]
        elif input_commit is not None:
            input_commit = [commit_from(input_commit)]

        output_commit = commit_from(output_commit) if output_commit is not None else None

        req = pps_proto.ListJobRequest(pipeline=pipeline, input_commit=input_commit,
                                   output_commit=output_commit, history=history)

        return self._pps_stub.ListJobStream(req, metadata=self.metadata)

    def flush_job(self, commits, pipeline_names=None):
        &#34;&#34;&#34;
        Blocks until all of the jobs which have a set of commits as
        provenance have finished. Yields `JobInfo` objects.

        Params:

        * commits: A list of tuples, strings, or `Commit` objects representing
        the commits to flush.
        * pipeline_names: An optional list of strings specifying pipeline
        names. If specified, only jobs within these pipelines will be flushed.
        &#34;&#34;&#34;

        commits = [commit_from(c) for c in commits]
        pipelines = [pps_proto.Pipeline(name=name) for name in pipeline_names] if pipeline_names is not None else None
        req = pps_proto.FlushJobRequest(commits=commits, to_pipelines=pipelines)
        return self._pps_stub.FlushJob(req)

    def delete_job(self, job_id):
        &#34;&#34;&#34;
        Deletes a job by its ID.

        Params:

        * job_id: The ID of the job to delete.
        &#34;&#34;&#34;

        req = pps_proto.DeleteJobRequest(job=pps_proto.Job(id=job_id))
        self._pps_stub.DeleteJob(req, metadata=self.metadata)

    def stop_job(self, job_id):
        &#34;&#34;&#34;
        Stops a job by its ID.

        Params:

        * job_id: The ID of the job to stop.
        &#34;&#34;&#34;

        req = pps_proto.StopJobRequest(job=pps_proto.Job(id=job_id))
        self._pps_stub.StopJob(req, metadata=self.metadata)

    def inspect_datum(self, job_id, datum_id):
        &#34;&#34;&#34;
        Inspects a datum. Returns a `DatumInfo` object.

        Params:

        * job_id: The ID of the job.
        * datum_id: The ID of the datum.
        &#34;&#34;&#34;

        req = pps_proto.InspectDatumRequest(datum=pps_proto.Datum(id=datum_id, job=pps_proto.Job(id=job_id)))
        return self._pps_stub.InspectDatum(req, metadata=self.metadata)

    def list_datum(self, job_id, page_size=None, page=None):
        &#34;&#34;&#34;
        Lists datums. Yields `ListDatumStreamResponse` objects.

        Params:

        * job_id: The ID of the job.
        * page_size: An optional int specifying the size of the page.
        * page: An optional int specifying the page number.
        &#34;&#34;&#34;

        req = pps_proto.ListDatumRequest(job=pps_proto.Job(id=job_id), page_size=page_size, page=page)
        return self._pps_stub.ListDatumStream(req, metadata=self.metadata)

    def restart_datum(self, job_id, data_filters=None):
        &#34;&#34;&#34;
        Restarts a datum.

        Params:

        * job_id: The ID of the job.
        * data_filters: An optional iterable of strings.
        &#34;&#34;&#34;

        req = pps_proto.RestartDatumRequest(job=pps_proto.Job(id=job_id), data_filters=data_filters)
        self._pps_stub.RestartDatum(req, metadata=self.metadata)

    def create_pipeline(self, pipeline_name, transform=None, parallelism_spec=None,
                        hashtree_spec=None, egress=None, update=None, output_branch=None,
                        scale_down_threshold=None, resource_requests=None,
                        resource_limits=None, input=None, description=None, cache_size=None,
                        enable_stats=None, reprocess=None, batch=None, max_queue_size=None,
                        service=None, chunk_spec=None, datum_timeout=None,
                        job_timeout=None, salt=None, standby=None, datum_tries=None,
                        scheduling_spec=None, pod_patch=None):
        &#34;&#34;&#34;
        Creates a pipeline. For more info, please refer to the pipeline spec
        document:
        http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html

        Params:

        * pipeline_name: A string representing the pipeline name.
        * transform: An optional `Transform` object.
        * parallelism_spec: An optional `ParallelismSpec` object.
        * hashtree_spec: An optional `HashtreeSpec` object.
        * egress: An optional `Egress` object.
        * update: An optional bool specifying whether this should behave as an
        upsert.
        * output_branch: An optional string representing the branch to output
        results on.
        * scale_down_threshold: An optional pps_proto.uf `Duration` object.
        * resource_requests: An optional `ResourceSpec` object.
        * resource_limits: An optional `ResourceSpec` object.
        * input: An optional `Input` object.
        * description: An optional string describing the pipeline.
        * cache_size: An optional string.
        * enable_stats: An optional bool.
        * reprocess: An optional bool. If true, pachyderm forces the pipeline
        to reprocess all datums. It only has meaning if `update` is `True`.
        * batch: An optional bool.
        * max_queue_size: An optional int.
        * service: An optional `Service` object.
        * chunk_spec: An optional `ChunkSpec` object.
        * datum_timeout: An optional pps_proto.uf `Duration` object.
        * job_timeout: An optional pps_proto.uf `Duration` object.
        * salt: An optional stirng.
        * standby: An optional bool.
        * datum_tries: An optional int.
        * scheduling_spec: An optional `SchedulingSpec` object.
        * pod_patch: An optional string.
        &#34;&#34;&#34;

        req = pps_proto.CreatePipelineRequest(
            pipeline=pps_proto.Pipeline(name=pipeline_name),
            transform=transform, parallelism_spec=parallelism_spec,
            hashtree_spec=hashtree_spec, egress=egress, update=update,
            output_branch=output_branch, scale_down_threshold=scale_down_threshold,
            resource_requests=resource_requests, resource_limits=resource_limits,
            input=input, description=description, cache_size=cache_size,
            enable_stats=enable_stats, reprocess=reprocess, batch=batch,
            max_queue_size=max_queue_size, service=service,
            chunk_spec=chunk_spec, datum_timeout=datum_timeout,
            job_timeout=job_timeout, salt=salt, standby=standby,
            datum_tries=datum_tries, scheduling_spec=scheduling_spec,
            pod_patch=pod_patch
        )
        self._pps_stub.CreatePipeline(req, metadata=self.metadata)

    def inspect_pipeline(self, pipeline_name, history=None):
        &#34;&#34;&#34;
        Inspects a pipeline. Returns a `PipelineInfo` object.

        Params:

        * pipeline_name: A string representing the pipeline name.
        * history: An optional int that indicates to return jobs from
        historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        &#34;&#34;&#34;

        pipeline = pps_proto.Pipeline(name=pipeline_name)

        if history is None:
            req = pps_proto.InspectPipelineRequest(pipeline=pipeline)
            return self._pps_stub.InspectPipeline(req, metadata=self.metadata)
        else:
            # `InspectPipeline` doesn&#39;t support history, but `ListPipeline`
            # with a pipeline filter does, so we use that here
            req = pps_proto.ListPipelineRequest(pipeline=pipeline, history=history)
            pipelines = self._pps_stub.ListPipeline(req, metadata=self.metadata).pipeline_info
            assert len(pipelines) &lt;= 1
            return pipelines[0] if len(pipelines) else None

    def list_pipeline(self, history=None):
        &#34;&#34;&#34;
        Lists pipelines. Returns a `PipelineInfos` object.

        Params:

        * pipeline_name: A string representing the pipeline name.
        * history: An optional int that indicates to return jobs from
        historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        &#34;&#34;&#34;
        req = pps_proto.ListPipelineRequest(history=history)
        return self._pps_stub.ListPipeline(req, metadata=self.metadata)

    def delete_pipeline(self, pipeline_name, force=None):
        &#34;&#34;&#34;
        Deletes a pipeline.

        Params:

        * pipeline_name: A string representing the pipeline name.
        * force: Whether to force delete.
        &#34;&#34;&#34;

        req = pps_proto.DeletePipelineRequest(pipeline=pps_proto.Pipeline(name=pipeline_name), force=force)
        self._pps_stub.DeletePipeline(req, metadata=self.metadata)

    def delete_all_pipelines(self, force=None):
        &#34;&#34;&#34;
        Deletes all pipelines.

        Params:

        * force: Whether to force delete.
        &#34;&#34;&#34;

        req = pps_proto.DeletePipelineRequest(all=True, force=force)
        self._pps_stub.DeletePipeline(req, metadata=self.metadata)

    def start_pipeline(self, pipeline_name):
        &#34;&#34;&#34;
        Starts a pipeline.

        Params:

        * pipeline_name: A string representing the pipeline name.
        &#34;&#34;&#34;

        req = pps_proto.StartPipelineRequest(pipeline=pps_proto.Pipeline(name=pipeline_name))
        self._pps_stub.StartPipeline(req, metadata=self.metadata)

    def stop_pipeline(self, pipeline_name):
        &#34;&#34;&#34;
        Stops a pipeline.

        Params:

        * pipeline_name: A string representing the pipeline name.
        &#34;&#34;&#34;
        req = pps_proto.StopPipelineRequest(pipeline=pps_proto.Pipeline(name=pipeline_name))
        self._pps_stub.StopPipeline(req, metadata=self.metadata)

    def run_pipeline(self, pipeline_name, provenance=None):
        &#34;&#34;&#34;
        Runs a pipeline.

        Params:

        * pipeline_name: A string representing the pipeline name.
        * provenance: An optional iterable of `CommitProvenance` objects
        representing the pipeline execution provenance.
        &#34;&#34;&#34;
        req = pps_proto.RunPipelineRequest(
            pipeline=pps_proto.Pipeline(name=pipeline_name),
            provenance=provenance,
        )
        self._pps_stub.RunPipeline(req, metadata=self.metadata)

    def delete_all(self):
        &#34;&#34;&#34;
        Deletes everything in pachyderm.
        &#34;&#34;&#34;
        req = pps_proto.google_dot_pps_proto.uf_dot_empty__pb2.Empty()
        self._pps_stub.DeleteAll(req, metadata=self.metadata)

    def get_pipeline_logs(self, pipeline_name, data_filters=None, master=None,
                          datum=None, follow=None, tail=None):
        &#34;&#34;&#34;
        Gets logs for a pipeline. Yields `LogMessage` objects.

        Params:

        * pipeline_name: A string representing a pipeline to get
        logs of.
        * data_filters: An optional iterable of strings specifying the names
        of input files from which we want processing logs. This may contain
        multiple files, to query pipelines that contain multiple inputs. Each
        filter may be an absolute path of a file within a pps repo, or it may
        be a hash for that file (to search for files at specific versions.)
        * master: An optional bool.
        * datum: An optional `Datum` object.
        * follow: An optional bool specifying whether logs should continue to
        stream forever.
        * tail: An optional int. If nonzero, the number of lines from the end
        of the logs to return.  Note: tail applies per container, so you will
        get tail * &lt;number of pods&gt; total lines back.
        &#34;&#34;&#34;

        req = pps_proto.GetLogsRequest(
            pipeline=pps_proto.Pipeline(name=pipeline_name),
            data_filters=data_filters, master=master, datum=datum,
            follow=follow, tail=tail,
        )
        return self._pps_stub.GetLogs(req, metadata=self.metadata)

    def get_job_logs(self, job_id, data_filters=None, datum=None, follow=None,
                     tail=None):
        &#34;&#34;&#34;
        Gets logs for a job. Yields `LogMessage` objects.

        Params:

        * job_id: A string representing a job to get logs of.
        * data_filters: An optional iterable of strings specifying the names
        of input files from which we want processing logs. This may contain
        multiple files, to query pipelines that contain multiple inputs. Each
        filter may be an absolute path of a file within a pps repo, or it may
        be a hash for that file (to search for files at specific versions.)
        * datum: An optional `Datum` object.
        * follow: An optional bool specifying whether logs should continue to
        stream forever.
        * tail: An optional int. If nonzero, the number of lines from the end
        of the logs to return.  Note: tail applies per container, so you will
        get tail * &lt;number of pods&gt; total lines back.
        &#34;&#34;&#34;

        req = pps_proto.GetLogsRequest(
            job=pps_proto.Job(id=job_id), data_filters=data_filters, datum=datum,
            follow=follow, tail=tail,
        )
        return self._pps_stub.GetLogs(req, metadata=self.metadata)

    def garbage_collect(self):
        &#34;&#34;&#34;
        Runs garbage collection.
        &#34;&#34;&#34;
        return self._pps_stub.GarbageCollect(pps_proto.GarbageCollectRequest(), metadata=self.metadata)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="python_pachyderm.client.Client.commit"><code class="name flex">
<span>def <span class="ident">commit</span></span>(<span>self, repo_name, branch=None, parent=None, description=None)</span>
</code></dt>
<dd>
<section class="desc"><p>A context manager for running operations within a commit.</p>
<p>Params:</p>
<ul>
<li>repo_name: A string specifying the name of the repo.</li>
<li>branch: A string specifying the branch name. This is a more
convenient way to build linear chains of commits. When a commit is
started with a non-empty branch the value of branch becomes an alias
for the created Commit. This enables a more intuitive access pattern.
When the commit is started on a branch the previous head of the branch
is used as the parent of the commit.</li>
<li>parent: An optional <code>Commit</code> object specifying the parent commit.
Upon creation the new commit will appear identical to the parent
commit, data can safely be added to the new commit without affecting
the contents of the parent commit.</li>
<li>description: An optional string describing the commit.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@contextmanager
def commit(self, repo_name, branch=None, parent=None, description=None):
    &#34;&#34;&#34;
    A context manager for running operations within a commit.

    Params:

    * repo_name: A string specifying the name of the repo.
    * branch: A string specifying the branch name. This is a more
    convenient way to build linear chains of commits. When a commit is
    started with a non-empty branch the value of branch becomes an alias
    for the created Commit. This enables a more intuitive access pattern.
    When the commit is started on a branch the previous head of the branch
    is used as the parent of the commit.
    * parent: An optional `Commit` object specifying the parent commit.
    Upon creation the new commit will appear identical to the parent
    commit, data can safely be added to the new commit without affecting
    the contents of the parent commit.
    * description: An optional string describing the commit.
    &#34;&#34;&#34;
    commit = self.start_commit(repo_name, branch, parent, description)
    try:
        yield commit
    finally:
        self.finish_commit(commit)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.copy_file"><code class="name flex">
<span>def <span class="ident">copy_file</span></span>(<span>self, source_commit, source_path, dest_commit, dest_path, overwrite=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Efficiently copies files already in PFS. Note that the destination
repo cannot be an output repo, or the copy operation will (as of
1.9.0) silently fail.</p>
<p>Params:</p>
<ul>
<li>source_commit: A tuple, string, or <code>Commit</code> object representing the
commit for the source file.</li>
<li>source_path: A string specifying the path of the source file.</li>
<li>dest_commit: A tuple, string, or <code>Commit</code> object representing the
commit for the destination file.</li>
<li>dest_path: A string specifying the path of the destination file.</li>
<li>overwrite: Am optional bool specifying whether to overwrite the
destination file if it already exists.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def copy_file(self, source_commit, source_path, dest_commit, dest_path, overwrite=None):
    &#34;&#34;&#34;
    Efficiently copies files already in PFS. Note that the destination
    repo cannot be an output repo, or the copy operation will (as of
    1.9.0) silently fail.

    Params:

    * source_commit: A tuple, string, or `Commit` object representing the
    commit for the source file.
    * source_path: A string specifying the path of the source file.
    * dest_commit: A tuple, string, or `Commit` object representing the
    commit for the destination file.
    * dest_path: A string specifying the path of the destination file.
    * overwrite: Am optional bool specifying whether to overwrite the
    destination file if it already exists.
    &#34;&#34;&#34;
    req = pfs_proto.CopyFileRequest(
        src=pfs_proto.File(commit=commit_from(source_commit), path=source_path),
        dst=pfs_proto.File(commit=commit_from(dest_commit), path=dest_path),
        overwrite=overwrite,
    )
    self._pfs_stub.CopyFile(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.create_branch"><code class="name flex">
<span>def <span class="ident">create_branch</span></span>(<span>self, repo_name, branch_name, commit=None, provenance=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Creates a new branch.</p>
<p>Params:
* repo_name: A string specifying the name of the repo.
* branch_name: A string specifying the new branch name.
* commit: An optional tuple, string, or <code>Commit</code> object representing
the head commit of the branch.
* provenance: An optional iterable of <code>Branch</code> objects representing
the branch provenance.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def create_branch(self, repo_name, branch_name, commit=None, provenance=None):
    &#34;&#34;&#34;
    Creates a new branch.

    Params:
    * repo_name: A string specifying the name of the repo.
    * branch_name: A string specifying the new branch name.
    * commit: An optional tuple, string, or `Commit` object representing
    the head commit of the branch.
    * provenance: An optional iterable of `Branch` objects representing
    the branch provenance.
    &#34;&#34;&#34;
    req = pfs_proto.CreateBranchRequest(
        branch=pfs_proto.Branch(repo=pfs_proto.Repo(name=repo_name), name=branch_name),
        head=commit_from(commit) if commit is not None else None,
        provenance=provenance,
    )
    self._pfs_stub.CreateBranch(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.create_pipeline"><code class="name flex">
<span>def <span class="ident">create_pipeline</span></span>(<span>self, pipeline_name, transform=None, parallelism_spec=None, hashtree_spec=None, egress=None, update=None, output_branch=None, scale_down_threshold=None, resource_requests=None, resource_limits=None, input=None, description=None, cache_size=None, enable_stats=None, reprocess=None, batch=None, max_queue_size=None, service=None, chunk_spec=None, datum_timeout=None, job_timeout=None, salt=None, standby=None, datum_tries=None, scheduling_spec=None, pod_patch=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Creates a pipeline. For more info, please refer to the pipeline spec
document:
<a href="http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html">http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html</a></p>
<p>Params:</p>
<ul>
<li>pipeline_name: A string representing the pipeline name.</li>
<li>transform: An optional <code>Transform</code> object.</li>
<li>parallelism_spec: An optional <code>ParallelismSpec</code> object.</li>
<li>hashtree_spec: An optional <code>HashtreeSpec</code> object.</li>
<li>egress: An optional <code>Egress</code> object.</li>
<li>update: An optional bool specifying whether this should behave as an
upsert.</li>
<li>output_branch: An optional string representing the branch to output
results on.</li>
<li>scale_down_threshold: An optional pps_proto.uf <code>Duration</code> object.</li>
<li>resource_requests: An optional <code>ResourceSpec</code> object.</li>
<li>resource_limits: An optional <code>ResourceSpec</code> object.</li>
<li>input: An optional <code>Input</code> object.</li>
<li>description: An optional string describing the pipeline.</li>
<li>cache_size: An optional string.</li>
<li>enable_stats: An optional bool.</li>
<li>reprocess: An optional bool. If true, pachyderm forces the pipeline
to reprocess all datums. It only has meaning if <code>update</code> is <code>True</code>.</li>
<li>batch: An optional bool.</li>
<li>max_queue_size: An optional int.</li>
<li>service: An optional <code>Service</code> object.</li>
<li>chunk_spec: An optional <code>ChunkSpec</code> object.</li>
<li>datum_timeout: An optional pps_proto.uf <code>Duration</code> object.</li>
<li>job_timeout: An optional pps_proto.uf <code>Duration</code> object.</li>
<li>salt: An optional stirng.</li>
<li>standby: An optional bool.</li>
<li>datum_tries: An optional int.</li>
<li>scheduling_spec: An optional <code>SchedulingSpec</code> object.</li>
<li>pod_patch: An optional string.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def create_pipeline(self, pipeline_name, transform=None, parallelism_spec=None,
                    hashtree_spec=None, egress=None, update=None, output_branch=None,
                    scale_down_threshold=None, resource_requests=None,
                    resource_limits=None, input=None, description=None, cache_size=None,
                    enable_stats=None, reprocess=None, batch=None, max_queue_size=None,
                    service=None, chunk_spec=None, datum_timeout=None,
                    job_timeout=None, salt=None, standby=None, datum_tries=None,
                    scheduling_spec=None, pod_patch=None):
    &#34;&#34;&#34;
    Creates a pipeline. For more info, please refer to the pipeline spec
    document:
    http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html

    Params:

    * pipeline_name: A string representing the pipeline name.
    * transform: An optional `Transform` object.
    * parallelism_spec: An optional `ParallelismSpec` object.
    * hashtree_spec: An optional `HashtreeSpec` object.
    * egress: An optional `Egress` object.
    * update: An optional bool specifying whether this should behave as an
    upsert.
    * output_branch: An optional string representing the branch to output
    results on.
    * scale_down_threshold: An optional pps_proto.uf `Duration` object.
    * resource_requests: An optional `ResourceSpec` object.
    * resource_limits: An optional `ResourceSpec` object.
    * input: An optional `Input` object.
    * description: An optional string describing the pipeline.
    * cache_size: An optional string.
    * enable_stats: An optional bool.
    * reprocess: An optional bool. If true, pachyderm forces the pipeline
    to reprocess all datums. It only has meaning if `update` is `True`.
    * batch: An optional bool.
    * max_queue_size: An optional int.
    * service: An optional `Service` object.
    * chunk_spec: An optional `ChunkSpec` object.
    * datum_timeout: An optional pps_proto.uf `Duration` object.
    * job_timeout: An optional pps_proto.uf `Duration` object.
    * salt: An optional stirng.
    * standby: An optional bool.
    * datum_tries: An optional int.
    * scheduling_spec: An optional `SchedulingSpec` object.
    * pod_patch: An optional string.
    &#34;&#34;&#34;

    req = pps_proto.CreatePipelineRequest(
        pipeline=pps_proto.Pipeline(name=pipeline_name),
        transform=transform, parallelism_spec=parallelism_spec,
        hashtree_spec=hashtree_spec, egress=egress, update=update,
        output_branch=output_branch, scale_down_threshold=scale_down_threshold,
        resource_requests=resource_requests, resource_limits=resource_limits,
        input=input, description=description, cache_size=cache_size,
        enable_stats=enable_stats, reprocess=reprocess, batch=batch,
        max_queue_size=max_queue_size, service=service,
        chunk_spec=chunk_spec, datum_timeout=datum_timeout,
        job_timeout=job_timeout, salt=salt, standby=standby,
        datum_tries=datum_tries, scheduling_spec=scheduling_spec,
        pod_patch=pod_patch
    )
    self._pps_stub.CreatePipeline(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.create_repo"><code class="name flex">
<span>def <span class="ident">create_repo</span></span>(<span>self, repo_name, description=None, update=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Creates a new <code>Repo</code> object in PFS with the given name. Repos are the
top level data object in PFS and should be used to store data of a
similar type. For example rather than having a single <code>Repo</code> for an
entire project you might have separate <code>Repo</code>s for logs, metrics,
database dumps etc.</p>
<p>Params:</p>
<ul>
<li>repo_name: Name of the repo.</li>
<li>description: An optional string describing the repo.</li>
<li>update: Whether to update if the repo already exists.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def create_repo(self, repo_name, description=None, update=None):
    &#34;&#34;&#34;
    Creates a new `Repo` object in PFS with the given name. Repos are the
    top level data object in PFS and should be used to store data of a
    similar type. For example rather than having a single `Repo` for an
    entire project you might have separate `Repo`s for logs, metrics,
    database dumps etc.

    Params:

    * repo_name: Name of the repo.
    * description: An optional string describing the repo.
    * update: Whether to update if the repo already exists.
    &#34;&#34;&#34;
    req = pfs_proto.CreateRepoRequest(
        repo=pfs_proto.Repo(name=repo_name),
        description=description,
        update=update
    )
    self._pfs_stub.CreateRepo(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.delete_all"><code class="name flex">
<span>def <span class="ident">delete_all</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Deletes everything in pachyderm.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def delete_all(self):
    &#34;&#34;&#34;
    Deletes everything in pachyderm.
    &#34;&#34;&#34;
    req = pps_proto.google_dot_pps_proto.uf_dot_empty__pb2.Empty()
    self._pps_stub.DeleteAll(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.delete_all_pipelines"><code class="name flex">
<span>def <span class="ident">delete_all_pipelines</span></span>(<span>self, force=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Deletes all pipelines.</p>
<p>Params:</p>
<ul>
<li>force: Whether to force delete.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def delete_all_pipelines(self, force=None):
    &#34;&#34;&#34;
    Deletes all pipelines.

    Params:

    * force: Whether to force delete.
    &#34;&#34;&#34;

    req = pps_proto.DeletePipelineRequest(all=True, force=force)
    self._pps_stub.DeletePipeline(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.delete_all_repos"><code class="name flex">
<span>def <span class="ident">delete_all_repos</span></span>(<span>self, force=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Deletes all repos.</p>
<p>Params:</p>
<ul>
<li>force: If set to true, the repo will be removed regardless of
errors. This argument should be used with care.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def delete_all_repos(self, force=None):
    &#34;&#34;&#34;
    Deletes all repos.

    Params:

    * force: If set to true, the repo will be removed regardless of
    errors. This argument should be used with care.
    &#34;&#34;&#34;

    req = pfs_proto.DeleteRepoRequest(force=force, all=True)
    self._pfs_stub.DeleteRepo(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.delete_branch"><code class="name flex">
<span>def <span class="ident">delete_branch</span></span>(<span>self, repo_name, branch_name, force=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Deletes a branch, but leaves the commits themselves intact. In other
words, those commits can still be accessed via commit IDs and other
branches they happen to be on.</p>
<p>Params:</p>
<ul>
<li>repo_name: A string specifying the repo name.</li>
<li>branch_name: A string specifying the name of the branch to delete.</li>
<li>force: A bool specifying whether to force the branch deletion.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def delete_branch(self, repo_name, branch_name, force=None):
    &#34;&#34;&#34;
    Deletes a branch, but leaves the commits themselves intact. In other
    words, those commits can still be accessed via commit IDs and other
    branches they happen to be on.

    Params:

    * repo_name: A string specifying the repo name.
    * branch_name: A string specifying the name of the branch to delete.
    * force: A bool specifying whether to force the branch deletion.
    &#34;&#34;&#34;
    branch = pfs_proto.Branch(repo=pfs_proto.Repo(name=repo_name), name=branch_name)
    req = pfs_proto.DeleteBranchRequest(branch=branch, force=force)
    self._pfs_stub.DeleteBranch(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.delete_commit"><code class="name flex">
<span>def <span class="ident">delete_commit</span></span>(<span>self, commit)</span>
</code></dt>
<dd>
<section class="desc"><p>Deletes a commit.</p>
<p>Params:</p>
<ul>
<li>commit: A tuple, string, or <code>Commit</code> object representing the commit.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def delete_commit(self, commit):
    &#34;&#34;&#34;
    Deletes a commit.

    Params:

    * commit: A tuple, string, or `Commit` object representing the commit.
    &#34;&#34;&#34;
    req = pfs_proto.DeleteCommitRequest(commit=commit_from(commit))
    self._pfs_stub.DeleteCommit(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.delete_file"><code class="name flex">
<span>def <span class="ident">delete_file</span></span>(<span>self, commit, path)</span>
</code></dt>
<dd>
<section class="desc"><p>Deletes a file from a Commit. DeleteFile leaves a tombstone in the
Commit, assuming the file isn't written to later attempting to get the
file from the finished commit will result in not found error. The file
will of course remain intact in the Commit's parent.</p>
<p>Params:</p>
<ul>
<li>commit: A tuple, string, or <code>Commit</code> object representing the commit.</li>
<li>path: The path to the file.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def delete_file(self, commit, path):
    &#34;&#34;&#34;
    Deletes a file from a Commit. DeleteFile leaves a tombstone in the
    Commit, assuming the file isn&#39;t written to later attempting to get the
    file from the finished commit will result in not found error. The file
    will of course remain intact in the Commit&#39;s parent.

    Params:

    * commit: A tuple, string, or `Commit` object representing the commit.
    * path: The path to the file.
    &#34;&#34;&#34;
    req = pfs_proto.DeleteFileRequest(file=pfs_proto.File(commit=commit_from(commit), path=path))
    self._pfs_stub.DeleteFile(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.delete_job"><code class="name flex">
<span>def <span class="ident">delete_job</span></span>(<span>self, job_id)</span>
</code></dt>
<dd>
<section class="desc"><p>Deletes a job by its ID.</p>
<p>Params:</p>
<ul>
<li>job_id: The ID of the job to delete.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def delete_job(self, job_id):
    &#34;&#34;&#34;
    Deletes a job by its ID.

    Params:

    * job_id: The ID of the job to delete.
    &#34;&#34;&#34;

    req = pps_proto.DeleteJobRequest(job=pps_proto.Job(id=job_id))
    self._pps_stub.DeleteJob(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.delete_pipeline"><code class="name flex">
<span>def <span class="ident">delete_pipeline</span></span>(<span>self, pipeline_name, force=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Deletes a pipeline.</p>
<p>Params:</p>
<ul>
<li>pipeline_name: A string representing the pipeline name.</li>
<li>force: Whether to force delete.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def delete_pipeline(self, pipeline_name, force=None):
    &#34;&#34;&#34;
    Deletes a pipeline.

    Params:

    * pipeline_name: A string representing the pipeline name.
    * force: Whether to force delete.
    &#34;&#34;&#34;

    req = pps_proto.DeletePipelineRequest(pipeline=pps_proto.Pipeline(name=pipeline_name), force=force)
    self._pps_stub.DeletePipeline(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.delete_repo"><code class="name flex">
<span>def <span class="ident">delete_repo</span></span>(<span>self, repo_name, force=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Deletes a repo and reclaims the storage space it was using.</p>
<p>Params:</p>
<ul>
<li>repo_name: The name of the repo.</li>
<li>force: If set to true, the repo will be removed regardless of
errors. This argument should be used with care.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def delete_repo(self, repo_name, force=None):
    &#34;&#34;&#34;
    Deletes a repo and reclaims the storage space it was using.

    Params:

    * repo_name: The name of the repo.
    * force: If set to true, the repo will be removed regardless of
    errors. This argument should be used with care.
    &#34;&#34;&#34;
    req = pfs_proto.DeleteRepoRequest(repo=pfs_proto.Repo(name=repo_name), force=force, all=False)
    self._pfs_stub.DeleteRepo(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.finish_commit"><code class="name flex">
<span>def <span class="ident">finish_commit</span></span>(<span>self, commit, description=None, tree_object_hashes=None, datum_object_hash=None, size_bytes=None, empty=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Ends the process of committing data to a Repo and persists the
Commit. Once a Commit is finished the data becomes immutable and
future attempts to write to it with PutFile will error.</p>
<p>Params:</p>
<ul>
<li>commit: A tuple, string, or <code>Commit</code> object representing the commit.</li>
<li>description: An optional string describing this commit.</li>
<li>tree_object_hashes: A list of zero or more strings specifying object
hashes.</li>
<li>datum_object_hash: An optional string specifying an object hash.</li>
<li>size_bytes: An optional int.</li>
<li>empty: An optional bool. If set, the commit will be closed (its
<code>finished</code> field will be set to the current time) but its <code>tree</code> will
be left nil.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def finish_commit(self, commit, description=None,
                  tree_object_hashes=None, datum_object_hash=None,
                  size_bytes=None, empty=None):
    &#34;&#34;&#34;
    Ends the process of committing data to a Repo and persists the
    Commit. Once a Commit is finished the data becomes immutable and
    future attempts to write to it with PutFile will error.

    Params:

    * commit: A tuple, string, or `Commit` object representing the commit.
    * description: An optional string describing this commit.
    * tree_object_hashes: A list of zero or more strings specifying object
    hashes.
    * datum_object_hash: An optional string specifying an object hash.
    * size_bytes: An optional int.
    * empty: An optional bool. If set, the commit will be closed (its
    `finished` field will be set to the current time) but its `tree` will
    be left nil.
    &#34;&#34;&#34;
    req = pfs_proto.FinishCommitRequest(
        commit=commit_from(commit),
        description=description,
        trees=[pfs_proto.Object(hash=h) for h in tree_object_hashes] if tree_object_hashes is not None else None,
        datums=pfs_proto.Object(hash=datum_object_hash) if datum_object_hash is not None else None,
        size_bytes=size_bytes,
        empty=empty,
    )
    return self._pfs_stub.FinishCommit(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.flush_commit"><code class="name flex">
<span>def <span class="ident">flush_commit</span></span>(<span>self, commits, repos=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Blocks until all of the commits which have a set of commits as
provenance have finished. For commits to be considered they must have
all of the specified commits as provenance. This in effect waits for
all of the jobs that are triggered by a set of commits to complete.
It returns an error if any of the commits it's waiting on are
cancelled due to one of the jobs encountering an error during runtime.
Note that it's never necessary to call FlushCommit to run jobs,
they'll run no matter what, FlushCommit just allows you to wait for
them to complete and see their output once they do. This returns an
iterator of CommitInfo objects.</p>
<p>Yields <code>CommitInfo</code> objects.</p>
<p>Params:</p>
<ul>
<li>commits: A list of tuples, strings, or <code>Commit</code> objects representing
the commits to flush.</li>
<li>repos: An optional list of strings specifying repo names. If
specified, only commits within these repos will be flushed.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def flush_commit(self, commits, repos=None):
    &#34;&#34;&#34;
    Blocks until all of the commits which have a set of commits as
    provenance have finished. For commits to be considered they must have
    all of the specified commits as provenance. This in effect waits for
    all of the jobs that are triggered by a set of commits to complete.
    It returns an error if any of the commits it&#39;s waiting on are
    cancelled due to one of the jobs encountering an error during runtime.
    Note that it&#39;s never necessary to call FlushCommit to run jobs,
    they&#39;ll run no matter what, FlushCommit just allows you to wait for
    them to complete and see their output once they do. This returns an
    iterator of CommitInfo objects.

    Yields `CommitInfo` objects.

    Params:

    * commits: A list of tuples, strings, or `Commit` objects representing
    the commits to flush.
    * repos: An optional list of strings specifying repo names. If
    specified, only commits within these repos will be flushed.
    &#34;&#34;&#34;
    to_repos = [pfs_proto.Repo(name=r) for r in repos] if repos is not None else None
    req = pfs_proto.FlushCommitRequest(commits=[commit_from(c) for c in commits],
                                   to_repos=to_repos)
    return self._pfs_stub.FlushCommit(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.flush_job"><code class="name flex">
<span>def <span class="ident">flush_job</span></span>(<span>self, commits, pipeline_names=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Blocks until all of the jobs which have a set of commits as
provenance have finished. Yields <code>JobInfo</code> objects.</p>
<p>Params:</p>
<ul>
<li>commits: A list of tuples, strings, or <code>Commit</code> objects representing
the commits to flush.</li>
<li>pipeline_names: An optional list of strings specifying pipeline
names. If specified, only jobs within these pipelines will be flushed.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def flush_job(self, commits, pipeline_names=None):
    &#34;&#34;&#34;
    Blocks until all of the jobs which have a set of commits as
    provenance have finished. Yields `JobInfo` objects.

    Params:

    * commits: A list of tuples, strings, or `Commit` objects representing
    the commits to flush.
    * pipeline_names: An optional list of strings specifying pipeline
    names. If specified, only jobs within these pipelines will be flushed.
    &#34;&#34;&#34;

    commits = [commit_from(c) for c in commits]
    pipelines = [pps_proto.Pipeline(name=name) for name in pipeline_names] if pipeline_names is not None else None
    req = pps_proto.FlushJobRequest(commits=commits, to_pipelines=pipelines)
    return self._pps_stub.FlushJob(req)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.garbage_collect"><code class="name flex">
<span>def <span class="ident">garbage_collect</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Runs garbage collection.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def garbage_collect(self):
    &#34;&#34;&#34;
    Runs garbage collection.
    &#34;&#34;&#34;
    return self._pps_stub.GarbageCollect(pps_proto.GarbageCollectRequest(), metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.get_file"><code class="name flex">
<span>def <span class="ident">get_file</span></span>(<span>self, commit, path, offset_bytes=None, size_bytes=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns an iterator of the contents of a file at a specific commit.</p>
<p>Params:</p>
<ul>
<li>commit: A tuple, string, or <code>Commit</code> object representing the commit.</li>
<li>path: A string specifying the path of the file.</li>
<li>offset_bytes: An optional int. Specifies a number of bytes that
should be skipped in the beginning of the file.</li>
<li>size_bytes: An optional int. limits the total amount of data
returned, note you will get fewer bytes than size if you pass a value
larger than the size of the file. If size is set to 0 then all of the
data will be returned.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_file(self, commit, path, offset_bytes=None, size_bytes=None):
    &#34;&#34;&#34;
    Returns an iterator of the contents of a file at a specific commit.

    Params:

    * commit: A tuple, string, or `Commit` object representing the commit.
    * path: A string specifying the path of the file.
    * offset_bytes: An optional int. Specifies a number of bytes that
    should be skipped in the beginning of the file.
    * size_bytes: An optional int. limits the total amount of data
    returned, note you will get fewer bytes than size if you pass a value
    larger than the size of the file. If size is set to 0 then all of the
    data will be returned.
    &#34;&#34;&#34;
    req = pfs_proto.GetFileRequest(
        file=pfs_proto.File(commit=commit_from(commit), path=path),
        offset_bytes=offset_bytes,
        size_bytes=size_bytes
    )
    res = self._pfs_stub.GetFile(req, metadata=self.metadata)
    for item in res:
        yield item.value</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.get_job_logs"><code class="name flex">
<span>def <span class="ident">get_job_logs</span></span>(<span>self, job_id, data_filters=None, datum=None, follow=None, tail=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Gets logs for a job. Yields <code>LogMessage</code> objects.</p>
<p>Params:</p>
<ul>
<li>job_id: A string representing a job to get logs of.</li>
<li>data_filters: An optional iterable of strings specifying the names
of input files from which we want processing logs. This may contain
multiple files, to query pipelines that contain multiple inputs. Each
filter may be an absolute path of a file within a pps repo, or it may
be a hash for that file (to search for files at specific versions.)</li>
<li>datum: An optional <code>Datum</code> object.</li>
<li>follow: An optional bool specifying whether logs should continue to
stream forever.</li>
<li>tail: An optional int. If nonzero, the number of lines from the end
of the logs to return.
Note: tail applies per container, so you will
get tail * <number of pods> total lines back.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_job_logs(self, job_id, data_filters=None, datum=None, follow=None,
                 tail=None):
    &#34;&#34;&#34;
    Gets logs for a job. Yields `LogMessage` objects.

    Params:

    * job_id: A string representing a job to get logs of.
    * data_filters: An optional iterable of strings specifying the names
    of input files from which we want processing logs. This may contain
    multiple files, to query pipelines that contain multiple inputs. Each
    filter may be an absolute path of a file within a pps repo, or it may
    be a hash for that file (to search for files at specific versions.)
    * datum: An optional `Datum` object.
    * follow: An optional bool specifying whether logs should continue to
    stream forever.
    * tail: An optional int. If nonzero, the number of lines from the end
    of the logs to return.  Note: tail applies per container, so you will
    get tail * &lt;number of pods&gt; total lines back.
    &#34;&#34;&#34;

    req = pps_proto.GetLogsRequest(
        job=pps_proto.Job(id=job_id), data_filters=data_filters, datum=datum,
        follow=follow, tail=tail,
    )
    return self._pps_stub.GetLogs(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.get_pipeline_logs"><code class="name flex">
<span>def <span class="ident">get_pipeline_logs</span></span>(<span>self, pipeline_name, data_filters=None, master=None, datum=None, follow=None, tail=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Gets logs for a pipeline. Yields <code>LogMessage</code> objects.</p>
<p>Params:</p>
<ul>
<li>pipeline_name: A string representing a pipeline to get
logs of.</li>
<li>data_filters: An optional iterable of strings specifying the names
of input files from which we want processing logs. This may contain
multiple files, to query pipelines that contain multiple inputs. Each
filter may be an absolute path of a file within a pps repo, or it may
be a hash for that file (to search for files at specific versions.)</li>
<li>master: An optional bool.</li>
<li>datum: An optional <code>Datum</code> object.</li>
<li>follow: An optional bool specifying whether logs should continue to
stream forever.</li>
<li>tail: An optional int. If nonzero, the number of lines from the end
of the logs to return.
Note: tail applies per container, so you will
get tail * <number of pods> total lines back.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_pipeline_logs(self, pipeline_name, data_filters=None, master=None,
                      datum=None, follow=None, tail=None):
    &#34;&#34;&#34;
    Gets logs for a pipeline. Yields `LogMessage` objects.

    Params:

    * pipeline_name: A string representing a pipeline to get
    logs of.
    * data_filters: An optional iterable of strings specifying the names
    of input files from which we want processing logs. This may contain
    multiple files, to query pipelines that contain multiple inputs. Each
    filter may be an absolute path of a file within a pps repo, or it may
    be a hash for that file (to search for files at specific versions.)
    * master: An optional bool.
    * datum: An optional `Datum` object.
    * follow: An optional bool specifying whether logs should continue to
    stream forever.
    * tail: An optional int. If nonzero, the number of lines from the end
    of the logs to return.  Note: tail applies per container, so you will
    get tail * &lt;number of pods&gt; total lines back.
    &#34;&#34;&#34;

    req = pps_proto.GetLogsRequest(
        pipeline=pps_proto.Pipeline(name=pipeline_name),
        data_filters=data_filters, master=master, datum=datum,
        follow=follow, tail=tail,
    )
    return self._pps_stub.GetLogs(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.get_remote_version"><code class="name flex">
<span>def <span class="ident">get_remote_version</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_remote_version(self):
    with closing(version_grpc.grpc.insecure_channel(self.address)) as channel:
        stub = version_grpc.APIStub(channel)
        return stub.GetVersion(version_grpc.google_dot_protobuf_dot_empty__pb2.Empty())</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.glob_file"><code class="name flex">
<span>def <span class="ident">glob_file</span></span>(<span>self, commit, pattern)</span>
</code></dt>
<dd>
<section class="desc"><p>Lists files that match a glob pattern. Yields <code>FileInfo</code> objects.</p>
<p>Params:</p>
<ul>
<li>commit: A tuple, string, or <code>Commit</code> object representing the commit.</li>
<li>pattern: A string representing a glob pattern.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def glob_file(self, commit, pattern):
    &#34;&#34;&#34;
    Lists files that match a glob pattern. Yields `FileInfo` objects.

    Params:

    * commit: A tuple, string, or `Commit` object representing the commit.
    * pattern: A string representing a glob pattern.
    &#34;&#34;&#34;

    req = pfs_proto.GlobFileRequest(commit=commit_from(commit), pattern=pattern)
    return self._pfs_stub.GlobFileStream(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.inspect_branch"><code class="name flex">
<span>def <span class="ident">inspect_branch</span></span>(<span>self, repo_name, branch_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Inspects a branch. Returns a <code>BranchInfo</code> object.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def inspect_branch(self, repo_name, branch_name):
    &#34;&#34;&#34;
    Inspects a branch. Returns a `BranchInfo` object.
    &#34;&#34;&#34;
    branch = pfs_proto.Branch(repo=pfs_proto.Repo(name=repo_name), name=branch_name)
    req = pfs_proto.InspectBranchRequest(branch=branch)
    return self._pfs_stub.InspectBranch(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.inspect_commit"><code class="name flex">
<span>def <span class="ident">inspect_commit</span></span>(<span>self, commit, block_state=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Inspects a commit. Returns a <code>CommitInfo</code> object.</p>
<p>Params:</p>
<ul>
<li>commit: A tuple, string, or <code>Commit</code> object representing the commit.</li>
<li>block_state: Causes inspect commit to block until the commit is in
the desired commit state.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def inspect_commit(self, commit, block_state=None):
    &#34;&#34;&#34;
    Inspects a commit. Returns a `CommitInfo` object.

    Params:

    * commit: A tuple, string, or `Commit` object representing the commit.
    * block_state: Causes inspect commit to block until the commit is in
    the desired commit state.
    &#34;&#34;&#34;
    req = pfs_proto.InspectCommitRequest(commit=commit_from(commit), block_state=block_state)
    return self._pfs_stub.InspectCommit(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.inspect_datum"><code class="name flex">
<span>def <span class="ident">inspect_datum</span></span>(<span>self, job_id, datum_id)</span>
</code></dt>
<dd>
<section class="desc"><p>Inspects a datum. Returns a <code>DatumInfo</code> object.</p>
<p>Params:</p>
<ul>
<li>job_id: The ID of the job.</li>
<li>datum_id: The ID of the datum.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def inspect_datum(self, job_id, datum_id):
    &#34;&#34;&#34;
    Inspects a datum. Returns a `DatumInfo` object.

    Params:

    * job_id: The ID of the job.
    * datum_id: The ID of the datum.
    &#34;&#34;&#34;

    req = pps_proto.InspectDatumRequest(datum=pps_proto.Datum(id=datum_id, job=pps_proto.Job(id=job_id)))
    return self._pps_stub.InspectDatum(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.inspect_file"><code class="name flex">
<span>def <span class="ident">inspect_file</span></span>(<span>self, commit, path)</span>
</code></dt>
<dd>
<section class="desc"><p>Inspects a file. Returns a <code>FileInfo</code> object.</p>
<p>Params:</p>
<ul>
<li>commit: A tuple, string, or <code>Commit</code> object representing the commit.</li>
<li>path: A string specifying the path to the file.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def inspect_file(self, commit, path):
    &#34;&#34;&#34;
    Inspects a file. Returns a `FileInfo` object.

    Params:

    * commit: A tuple, string, or `Commit` object representing the commit.
    * path: A string specifying the path to the file.
    &#34;&#34;&#34;
    req = pfs_proto.InspectFileRequest(file=pfs_proto.File(commit=commit_from(commit), path=path))
    return self._pfs_stub.InspectFile(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.inspect_job"><code class="name flex">
<span>def <span class="ident">inspect_job</span></span>(<span>self, job_id, block_state=None, output_commit=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Inspects a job with a given ID. Returns a <code>JobInfo</code>.</p>
<p>Params:</p>
<ul>
<li>job_id: The ID of the job to inspect.</li>
<li>block_state: If true, block until the job completes.</li>
<li>output_commit: An optional tuple, string, or <code>Commit</code> object
representing an output commit to filter on.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def inspect_job(self, job_id, block_state=None, output_commit=None):
    &#34;&#34;&#34;
    Inspects a job with a given ID. Returns a `JobInfo`.

    Params:

    * job_id: The ID of the job to inspect.
    * block_state: If true, block until the job completes.
    * output_commit: An optional tuple, string, or `Commit` object
    representing an output commit to filter on.
    &#34;&#34;&#34;

    output_commit = commit_from(output_commit) if output_commit is not None else None
    req = pps_proto.InspectJobRequest(job=pps_proto.Job(id=job_id), block_state=block_state, output_commit=output_commit)
    return self._pps_stub.InspectJob(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.inspect_pipeline"><code class="name flex">
<span>def <span class="ident">inspect_pipeline</span></span>(<span>self, pipeline_name, history=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Inspects a pipeline. Returns a <code>PipelineInfo</code> object.</p>
<p>Params:</p>
<ul>
<li>pipeline_name: A string representing the pipeline name.</li>
<li>history: An optional int that indicates to return jobs from
historical versions of pipelines. Semantics are:<ul>
<li>0: Return jobs from the current version of the pipeline or
pipelines.</li>
<li>1: Return the above and jobs from the next most recent version</li>
<li>2: etc.</li>
<li>-1: Return jobs from all historical versions.</li>
</ul>
</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def inspect_pipeline(self, pipeline_name, history=None):
    &#34;&#34;&#34;
    Inspects a pipeline. Returns a `PipelineInfo` object.

    Params:

    * pipeline_name: A string representing the pipeline name.
    * history: An optional int that indicates to return jobs from
    historical versions of pipelines. Semantics are:
        * 0: Return jobs from the current version of the pipeline or
          pipelines.
        * 1: Return the above and jobs from the next most recent version
        * 2: etc.
        * -1: Return jobs from all historical versions.
    &#34;&#34;&#34;

    pipeline = pps_proto.Pipeline(name=pipeline_name)

    if history is None:
        req = pps_proto.InspectPipelineRequest(pipeline=pipeline)
        return self._pps_stub.InspectPipeline(req, metadata=self.metadata)
    else:
        # `InspectPipeline` doesn&#39;t support history, but `ListPipeline`
        # with a pipeline filter does, so we use that here
        req = pps_proto.ListPipelineRequest(pipeline=pipeline, history=history)
        pipelines = self._pps_stub.ListPipeline(req, metadata=self.metadata).pipeline_info
        assert len(pipelines) &lt;= 1
        return pipelines[0] if len(pipelines) else None</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.inspect_repo"><code class="name flex">
<span>def <span class="ident">inspect_repo</span></span>(<span>self, repo_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns info about a specific repo. Returns a <code>RepoInfo</code> object.</p>
<p>Params:
* repo_name: Name of the repo.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def inspect_repo(self, repo_name):
    &#34;&#34;&#34;
    Returns info about a specific repo. Returns a `RepoInfo` object.

    Params:
    * repo_name: Name of the repo.
    &#34;&#34;&#34;
    req = pfs_proto.InspectRepoRequest(repo=pfs_proto.Repo(name=repo_name))
    return self._pfs_stub.InspectRepo(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.list_branch"><code class="name flex">
<span>def <span class="ident">list_branch</span></span>(<span>self, repo_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Lists the active branch objects on a repo. Returns a list of
<code>BranchInfo</code> objects.</p>
<p>Params:</p>
<ul>
<li>repo_name: A string specifying the repo name.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def list_branch(self, repo_name):
    &#34;&#34;&#34;
    Lists the active branch objects on a repo. Returns a list of
    `BranchInfo` objects.

    Params:

    * repo_name: A string specifying the repo name.
    &#34;&#34;&#34;
    req = pfs_proto.ListBranchRequest(repo=pfs_proto.Repo(name=repo_name))
    res = self._pfs_stub.ListBranch(req, metadata=self.metadata)
    return res.branch_info</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.list_commit"><code class="name flex">
<span>def <span class="ident">list_commit</span></span>(<span>self, repo_name, to_commit=None, from_commit=None, number=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Lists commits. Yields <code>CommitInfo</code> objects.</p>
<p>Params:</p>
<ul>
<li>repo_name: If only <code>repo_name</code> is given, all commits in the repo are
returned.</li>
<li>to_commit: Optional. Only the ancestors of <code>to</code>, including <code>to</code>
itself, are considered.</li>
<li>from_commit: Optional. Only the descendants of <code>from</code>, including
<code>from</code> itself, are considered.</li>
<li>number: Optional. Determines how many commits are returned.
If
<code>number</code> is 0, all commits that match the aforementioned criteria are
returned.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def list_commit(self, repo_name, to_commit=None, from_commit=None, number=None):
    &#34;&#34;&#34;
    Lists commits. Yields `CommitInfo` objects.

    Params:

    * repo_name: If only `repo_name` is given, all commits in the repo are
    returned.
    * to_commit: Optional. Only the ancestors of `to`, including `to`
    itself, are considered.
    * from_commit: Optional. Only the descendants of `from`, including
    `from` itself, are considered.
    * number: Optional. Determines how many commits are returned.  If
    `number` is 0, all commits that match the aforementioned criteria are
    returned.
    &#34;&#34;&#34;
    req = pfs_proto.ListCommitRequest(repo=pfs_proto.Repo(name=repo_name), number=number)
    if to_commit is not None:
        req.to.CopyFrom(commit_from(to_commit))
    if from_commit is not None:
        getattr(req, &#39;from&#39;).CopyFrom(commit_from(from_commit))
    return self._pfs_stub.ListCommitStream(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.list_datum"><code class="name flex">
<span>def <span class="ident">list_datum</span></span>(<span>self, job_id, page_size=None, page=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Lists datums. Yields <code>ListDatumStreamResponse</code> objects.</p>
<p>Params:</p>
<ul>
<li>job_id: The ID of the job.</li>
<li>page_size: An optional int specifying the size of the page.</li>
<li>page: An optional int specifying the page number.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def list_datum(self, job_id, page_size=None, page=None):
    &#34;&#34;&#34;
    Lists datums. Yields `ListDatumStreamResponse` objects.

    Params:

    * job_id: The ID of the job.
    * page_size: An optional int specifying the size of the page.
    * page: An optional int specifying the page number.
    &#34;&#34;&#34;

    req = pps_proto.ListDatumRequest(job=pps_proto.Job(id=job_id), page_size=page_size, page=page)
    return self._pps_stub.ListDatumStream(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.list_file"><code class="name flex">
<span>def <span class="ident">list_file</span></span>(<span>self, commit, path, history=None, include_contents=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Lists the files in a directory.</p>
<p>Params:</p>
<ul>
<li>commit: A tuple, string, or <code>Commit</code> object representing the commit.</li>
<li>path: The path to the directory.</li>
<li>history: An optional int that indicates to return jobs from
historical versions of pipelines. Semantics are:
0: Return jobs from the current version of the pipeline or pipelines.
1: Return the above and jobs from the next most recent version
2: etc.
-1: Return jobs from all historical versions.</li>
<li>include_contents: An optional bool. If <code>True</code>, file contents are
included.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def list_file(self, commit, path, history=None, include_contents=None):
    &#34;&#34;&#34;
    Lists the files in a directory.

    Params:

    * commit: A tuple, string, or `Commit` object representing the commit.
    * path: The path to the directory.
    * history: An optional int that indicates to return jobs from
    historical versions of pipelines. Semantics are:
     0: Return jobs from the current version of the pipeline or pipelines.
     1: Return the above and jobs from the next most recent version
     2: etc.
    -1: Return jobs from all historical versions.
    * include_contents: An optional bool. If `True`, file contents are
    included.
    &#34;&#34;&#34;

    req = pfs_proto.ListFileRequest(
        file=pfs_proto.File(commit=commit_from(commit), path=path),
        history=history,
        full=include_contents,
    )

    return self._pfs_stub.ListFileStream(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.list_job"><code class="name flex">
<span>def <span class="ident">list_job</span></span>(<span>self, pipeline_name=None, input_commit=None, output_commit=None, history=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Lists jobs. Yields <code>JobInfo</code> objects.</p>
<p>Params:</p>
<ul>
<li>pipeline_name: An optional string representing a pipeline name to
filter on.</li>
<li>input_commit: An optional list of tuples, strings, or <code>Commit</code>
objects representing input commits to filter on.</li>
<li>output_commit: An optional tuple, string, or <code>Commit</code> object
representing an output commit to filter on.</li>
<li>history: An optional int that indicates to return jobs from
historical versions of pipelines. Semantics are:<ul>
<li>0: Return jobs from the current version of the pipeline or
pipelines.</li>
<li>1: Return the above and jobs from the next most recent version</li>
<li>2: etc.</li>
<li>-1: Return jobs from all historical versions.</li>
</ul>
</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def list_job(self, pipeline_name=None, input_commit=None, output_commit=None, history=None):
    &#34;&#34;&#34;
    Lists jobs. Yields `JobInfo` objects.

    Params:

    * pipeline_name: An optional string representing a pipeline name to
    filter on.
    * input_commit: An optional list of tuples, strings, or `Commit`
    objects representing input commits to filter on.
    * output_commit: An optional tuple, string, or `Commit` object
    representing an output commit to filter on.
    * history: An optional int that indicates to return jobs from
      historical versions of pipelines. Semantics are:
        * 0: Return jobs from the current version of the pipeline or
          pipelines.
        * 1: Return the above and jobs from the next most recent version
        * 2: etc.
        * -1: Return jobs from all historical versions.
    &#34;&#34;&#34;

    pipeline = pps_proto.Pipeline(name=pipeline_name) if pipeline_name is not None else None

    if isinstance(input_commit, list):
        input_commit = [commit_from(ic) for ic in input_commit]
    elif input_commit is not None:
        input_commit = [commit_from(input_commit)]

    output_commit = commit_from(output_commit) if output_commit is not None else None

    req = pps_proto.ListJobRequest(pipeline=pipeline, input_commit=input_commit,
                               output_commit=output_commit, history=history)

    return self._pps_stub.ListJobStream(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.list_pipeline"><code class="name flex">
<span>def <span class="ident">list_pipeline</span></span>(<span>self, history=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Lists pipelines. Returns a <code>PipelineInfos</code> object.</p>
<p>Params:</p>
<ul>
<li>pipeline_name: A string representing the pipeline name.</li>
<li>history: An optional int that indicates to return jobs from
historical versions of pipelines. Semantics are:<ul>
<li>0: Return jobs from the current version of the pipeline or
pipelines.</li>
<li>1: Return the above and jobs from the next most recent version</li>
<li>2: etc.</li>
<li>-1: Return jobs from all historical versions.</li>
</ul>
</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def list_pipeline(self, history=None):
    &#34;&#34;&#34;
    Lists pipelines. Returns a `PipelineInfos` object.

    Params:

    * pipeline_name: A string representing the pipeline name.
    * history: An optional int that indicates to return jobs from
    historical versions of pipelines. Semantics are:
        * 0: Return jobs from the current version of the pipeline or
          pipelines.
        * 1: Return the above and jobs from the next most recent version
        * 2: etc.
        * -1: Return jobs from all historical versions.
    &#34;&#34;&#34;
    req = pps_proto.ListPipelineRequest(history=history)
    return self._pps_stub.ListPipeline(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.list_repo"><code class="name flex">
<span>def <span class="ident">list_repo</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns info about all repos, as a list of <code>RepoInfo</code> objects.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def list_repo(self):
    &#34;&#34;&#34;
    Returns info about all repos, as a list of `RepoInfo` objects.
    &#34;&#34;&#34;
    req = pfs_proto.ListRepoRequest()
    res = self._pfs_stub.ListRepo(req, metadata=self.metadata)
    return res.repo_info</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.put_file_bytes"><code class="name flex">
<span>def <span class="ident">put_file_bytes</span></span>(<span>self, commit, path, value, delimiter=None, target_file_datums=None, target_file_bytes=None, overwrite_index=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Uploads a binary bytes array as file(s) in a certain path.</p>
<p>Params:</p>
<ul>
<li>commit: A tuple, string, or <code>Commit</code> object representing the commit.</li>
<li>path: A string specifying the path in the repo the file(s) will be
written to.</li>
<li>value: The file contents as bytes, represented as a file-like
object, bytestring, or iterator of bytestrings.</li>
<li>delimiter: Optional. causes data to be broken up into separate files
with <code>path</code> as a prefix.</li>
<li>target_file_datums: An optional int. Specifies the target number of
datums in each written file. It may be lower if data does not split
evenly, but will never be higher, unless the value is 0.</li>
<li>target_file_bytes: An optional int. Specifies the target number of
bytes in each written file, files may have more or fewer bytes than
the target.</li>
<li>overwrite_index: An optional <code>OverwriteIndex</code> object. This is the
object index where the write starts from.
All existing objects
starting from the index are deleted.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def put_file_bytes(self, commit, path, value, delimiter=None,
                   target_file_datums=None, target_file_bytes=None, overwrite_index=None):
    &#34;&#34;&#34;
    Uploads a binary bytes array as file(s) in a certain path.

    Params:

    * commit: A tuple, string, or `Commit` object representing the commit.
    * path: A string specifying the path in the repo the file(s) will be
    written to.
    * value: The file contents as bytes, represented as a file-like
    object, bytestring, or iterator of bytestrings.
    * delimiter: Optional. causes data to be broken up into separate files
    with `path` as a prefix.
    * target_file_datums: An optional int. Specifies the target number of
    datums in each written file. It may be lower if data does not split
    evenly, but will never be higher, unless the value is 0.
    * target_file_bytes: An optional int. Specifies the target number of
    bytes in each written file, files may have more or fewer bytes than
    the target.
    * overwrite_index: An optional `OverwriteIndex` object. This is the
    object index where the write starts from.  All existing objects
    starting from the index are deleted.
    &#34;&#34;&#34;

    overwrite_index = pfs_proto.OverwriteIndex(index=overwrite_index) if overwrite_index is not None else None

    if hasattr(value, &#34;read&#34;):
        def wrap(value):
            for i in itertools.count():
                chunk = value.read(BUFFER_SIZE)

                if len(chunk) == 0:
                    return

                if i == 0:
                    yield pfs_proto.PutFileRequest(
                        file=pfs_proto.File(commit=commit_from(commit), path=path),
                        value=chunk,
                        delimiter=delimiter,
                        target_file_datums=target_file_datums,
                        target_file_bytes=target_file_bytes,
                        overwrite_index=overwrite_index
                    )
                else:
                    yield pfs_proto.PutFileRequest(value=chunk)
    elif isinstance(value, collections.Iterable) and not isinstance(value, (str, bytes)):
        def wrap(value):
            for i, chunk in enumerate(value):
                if i == 0:
                    yield pfs_proto.PutFileRequest(
                        file=pfs_proto.File(commit=commit_from(commit), path=path),
                        value=chunk,
                        delimiter=delimiter,
                        target_file_datums=target_file_datums,
                        target_file_bytes=target_file_bytes,
                        overwrite_index=overwrite_index
                    )
                else:
                    yield pfs_proto.PutFileRequest(value=chunk)
    else:
        def wrap(value):
            yield pfs_proto.PutFileRequest(
                file=pfs_proto.File(commit=commit_from(commit), path=path),
                value=value[:BUFFER_SIZE],
                delimiter=delimiter,
                target_file_datums=target_file_datums,
                target_file_bytes=target_file_bytes,
                overwrite_index=overwrite_index
            )

            for i in range(BUFFER_SIZE, len(value), BUFFER_SIZE):
                yield pfs_proto.PutFileRequest(
                    value=value[i:i + BUFFER_SIZE],
                    overwrite_index=overwrite_index
                )

    self._pfs_stub.PutFile(wrap(value), metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.put_file_url"><code class="name flex">
<span>def <span class="ident">put_file_url</span></span>(<span>self, commit, path, url, recursive=None, overwrite_index=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Puts a file using the content found at a URL. The URL is sent to the
server which performs the request. Note that this is not a standard
PFS function.</p>
<p>Params:</p>
<ul>
<li>commit: A tuple, string, or <code>Commit</code> object representing the commit.</li>
<li>path: A string specifying the path to the file.</li>
<li>url: A string specifying the url of the file to put.</li>
<li>recursive: allow for recursive scraping of some types URLs, for
example on s3:// URLs.</li>
<li>overwrite_index: An optional <code>OverwriteIndex</code> object. This is the
object index where the write starts from.
All existing objects
starting from the index are deleted.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def put_file_url(self, commit, path, url, recursive=None, overwrite_index=None):
    &#34;&#34;&#34;
    Puts a file using the content found at a URL. The URL is sent to the
    server which performs the request. Note that this is not a standard
    PFS function.

    Params:

    * commit: A tuple, string, or `Commit` object representing the commit.
    * path: A string specifying the path to the file.
    * url: A string specifying the url of the file to put.
    * recursive: allow for recursive scraping of some types URLs, for
    example on s3:// URLs.
    * overwrite_index: An optional `OverwriteIndex` object. This is the
    object index where the write starts from.  All existing objects
    starting from the index are deleted.
    &#34;&#34;&#34;

    overwrite_index = pfs_proto.OverwriteIndex(index=overwrite_index) if overwrite_index is not None else None

    req = iter([
        pfs_proto.PutFileRequest(
            file=pfs_proto.File(commit=commit_from(commit), path=path),
            url=url,
            recursive=recursive,
            overwrite_index=overwrite_index
        )
    ])
    self._pfs_stub.PutFile(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.restart_datum"><code class="name flex">
<span>def <span class="ident">restart_datum</span></span>(<span>self, job_id, data_filters=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Restarts a datum.</p>
<p>Params:</p>
<ul>
<li>job_id: The ID of the job.</li>
<li>data_filters: An optional iterable of strings.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def restart_datum(self, job_id, data_filters=None):
    &#34;&#34;&#34;
    Restarts a datum.

    Params:

    * job_id: The ID of the job.
    * data_filters: An optional iterable of strings.
    &#34;&#34;&#34;

    req = pps_proto.RestartDatumRequest(job=pps_proto.Job(id=job_id), data_filters=data_filters)
    self._pps_stub.RestartDatum(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.run_pipeline"><code class="name flex">
<span>def <span class="ident">run_pipeline</span></span>(<span>self, pipeline_name, provenance=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Runs a pipeline.</p>
<p>Params:</p>
<ul>
<li>pipeline_name: A string representing the pipeline name.</li>
<li>provenance: An optional iterable of <code>CommitProvenance</code> objects
representing the pipeline execution provenance.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def run_pipeline(self, pipeline_name, provenance=None):
    &#34;&#34;&#34;
    Runs a pipeline.

    Params:

    * pipeline_name: A string representing the pipeline name.
    * provenance: An optional iterable of `CommitProvenance` objects
    representing the pipeline execution provenance.
    &#34;&#34;&#34;
    req = pps_proto.RunPipelineRequest(
        pipeline=pps_proto.Pipeline(name=pipeline_name),
        provenance=provenance,
    )
    self._pps_stub.RunPipeline(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.start_commit"><code class="name flex">
<span>def <span class="ident">start_commit</span></span>(<span>self, repo_name, branch=None, parent=None, description=None, provenance=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Begins the process of committing data to a Repo. Once started you can
write to the Commit with PutFile and when all the data has been
written you must finish the Commit with FinishCommit. NOTE, data is
not persisted until FinishCommit is called. A Commit object is
returned.</p>
<p>Params:</p>
<ul>
<li>repo_name: A string specifying the name of the repo.</li>
<li>branch: A string specifying the branch name. This is a more
convenient way to build linear chains of commits. When a commit is
started with a non-empty branch the value of branch becomes an alias
for the created Commit. This enables a more intuitive access pattern.
When the commit is started on a branch the previous head of the branch
is used as the parent of the commit.</li>
<li>parent: An optional <code>Commit</code> object specifying the parent commit.
Upon creation the new commit will appear identical to the parent
commit, data can safely be added to the new commit without affecting
the contents of the parent commit.</li>
<li>description: An optional string describing the commit.</li>
<li>provenance: An optional iterable of <code>CommitProvenance</code> objects
specifying the commit provenance.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def start_commit(self, repo_name, branch=None, parent=None, description=None, provenance=None):
    &#34;&#34;&#34;
    Begins the process of committing data to a Repo. Once started you can
    write to the Commit with PutFile and when all the data has been
    written you must finish the Commit with FinishCommit. NOTE, data is
    not persisted until FinishCommit is called. A Commit object is
    returned.

    Params:

    * repo_name: A string specifying the name of the repo.
    * branch: A string specifying the branch name. This is a more
    convenient way to build linear chains of commits. When a commit is
    started with a non-empty branch the value of branch becomes an alias
    for the created Commit. This enables a more intuitive access pattern.
    When the commit is started on a branch the previous head of the branch
    is used as the parent of the commit.
    * parent: An optional `Commit` object specifying the parent commit.
    Upon creation the new commit will appear identical to the parent
    commit, data can safely be added to the new commit without affecting
    the contents of the parent commit.
    * description: An optional string describing the commit.
    * provenance: An optional iterable of `CommitProvenance` objects
    specifying the commit provenance.
    &#34;&#34;&#34;
    req = pfs_proto.StartCommitRequest(
        parent=pfs_proto.Commit(repo=pfs_proto.Repo(name=repo_name), id=parent),
        branch=branch,
        description=description,
        provenance=provenance,
    )
    return self._pfs_stub.StartCommit(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.start_pipeline"><code class="name flex">
<span>def <span class="ident">start_pipeline</span></span>(<span>self, pipeline_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Starts a pipeline.</p>
<p>Params:</p>
<ul>
<li>pipeline_name: A string representing the pipeline name.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def start_pipeline(self, pipeline_name):
    &#34;&#34;&#34;
    Starts a pipeline.

    Params:

    * pipeline_name: A string representing the pipeline name.
    &#34;&#34;&#34;

    req = pps_proto.StartPipelineRequest(pipeline=pps_proto.Pipeline(name=pipeline_name))
    self._pps_stub.StartPipeline(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.stop_job"><code class="name flex">
<span>def <span class="ident">stop_job</span></span>(<span>self, job_id)</span>
</code></dt>
<dd>
<section class="desc"><p>Stops a job by its ID.</p>
<p>Params:</p>
<ul>
<li>job_id: The ID of the job to stop.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def stop_job(self, job_id):
    &#34;&#34;&#34;
    Stops a job by its ID.

    Params:

    * job_id: The ID of the job to stop.
    &#34;&#34;&#34;

    req = pps_proto.StopJobRequest(job=pps_proto.Job(id=job_id))
    self._pps_stub.StopJob(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.stop_pipeline"><code class="name flex">
<span>def <span class="ident">stop_pipeline</span></span>(<span>self, pipeline_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Stops a pipeline.</p>
<p>Params:</p>
<ul>
<li>pipeline_name: A string representing the pipeline name.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def stop_pipeline(self, pipeline_name):
    &#34;&#34;&#34;
    Stops a pipeline.

    Params:

    * pipeline_name: A string representing the pipeline name.
    &#34;&#34;&#34;
    req = pps_proto.StopPipelineRequest(pipeline=pps_proto.Pipeline(name=pipeline_name))
    self._pps_stub.StopPipeline(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.subscribe_commit"><code class="name flex">
<span>def <span class="ident">subscribe_commit</span></span>(<span>self, repo_name, branch, from_commit_id=None, state=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Yields <code>CommitInfo</code> objects as commits occur.</p>
<p>Params:</p>
<ul>
<li>repo_name: A string specifying the name of the repo.</li>
<li>branch: A string specifying branch to subscribe to.</li>
<li>from_commit_id: An optional string specifying the commit ID. Only
commits created since this commit are returned.</li>
<li>state: The commit state to filter on.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def subscribe_commit(self, repo_name, branch, from_commit_id=None, state=None):
    &#34;&#34;&#34;
    Yields `CommitInfo` objects as commits occur.

    Params:

    * repo_name: A string specifying the name of the repo.
    * branch: A string specifying branch to subscribe to.
    * from_commit_id: An optional string specifying the commit ID. Only
    commits created since this commit are returned.
    * state: The commit state to filter on.
    &#34;&#34;&#34;
    repo = pfs_proto.Repo(name=repo_name)
    req = pfs_proto.SubscribeCommitRequest(repo=repo, branch=branch, state=state)
    if from_commit_id is not None:
        getattr(req, &#39;from&#39;).CopyFrom(pfs_proto.Commit(repo=repo, id=from_commit_id))
    return self._pfs_stub.SubscribeCommit(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.client.Client.walk_file"><code class="name flex">
<span>def <span class="ident">walk_file</span></span>(<span>self, commit, path)</span>
</code></dt>
<dd>
<section class="desc"><p>Walks over all descendant files in a directory. Returns a generator of
<code>FileInfo</code> objects.</p>
<p>Params:</p>
<ul>
<li>commit: A tuple, string, or <code>Commit</code> object representing the commit.</li>
<li>path: The path to the directory.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def walk_file(self, commit, path):
    &#34;&#34;&#34;
    Walks over all descendant files in a directory. Returns a generator of
    `FileInfo` objects.

    Params:

    * commit: A tuple, string, or `Commit` object representing the commit.
    * path: The path to the directory.
    &#34;&#34;&#34;
    commit = commit_from(commit)
    f = pfs_proto.File(commit=commit_from(commit), path=path)
    req = pfs_proto.WalkFileRequest(file=f)
    return self._pfs_stub.WalkFile(req, metadata=self.metadata)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="python_pachyderm" href="index.html">python_pachyderm</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="python_pachyderm.client.commit_from" href="#python_pachyderm.client.commit_from">commit_from</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="python_pachyderm.client.Client" href="#python_pachyderm.client.Client">Client</a></code></h4>
<ul class="">
<li><code><a title="python_pachyderm.client.Client.commit" href="#python_pachyderm.client.Client.commit">commit</a></code></li>
<li><code><a title="python_pachyderm.client.Client.copy_file" href="#python_pachyderm.client.Client.copy_file">copy_file</a></code></li>
<li><code><a title="python_pachyderm.client.Client.create_branch" href="#python_pachyderm.client.Client.create_branch">create_branch</a></code></li>
<li><code><a title="python_pachyderm.client.Client.create_pipeline" href="#python_pachyderm.client.Client.create_pipeline">create_pipeline</a></code></li>
<li><code><a title="python_pachyderm.client.Client.create_repo" href="#python_pachyderm.client.Client.create_repo">create_repo</a></code></li>
<li><code><a title="python_pachyderm.client.Client.delete_all" href="#python_pachyderm.client.Client.delete_all">delete_all</a></code></li>
<li><code><a title="python_pachyderm.client.Client.delete_all_pipelines" href="#python_pachyderm.client.Client.delete_all_pipelines">delete_all_pipelines</a></code></li>
<li><code><a title="python_pachyderm.client.Client.delete_all_repos" href="#python_pachyderm.client.Client.delete_all_repos">delete_all_repos</a></code></li>
<li><code><a title="python_pachyderm.client.Client.delete_branch" href="#python_pachyderm.client.Client.delete_branch">delete_branch</a></code></li>
<li><code><a title="python_pachyderm.client.Client.delete_commit" href="#python_pachyderm.client.Client.delete_commit">delete_commit</a></code></li>
<li><code><a title="python_pachyderm.client.Client.delete_file" href="#python_pachyderm.client.Client.delete_file">delete_file</a></code></li>
<li><code><a title="python_pachyderm.client.Client.delete_job" href="#python_pachyderm.client.Client.delete_job">delete_job</a></code></li>
<li><code><a title="python_pachyderm.client.Client.delete_pipeline" href="#python_pachyderm.client.Client.delete_pipeline">delete_pipeline</a></code></li>
<li><code><a title="python_pachyderm.client.Client.delete_repo" href="#python_pachyderm.client.Client.delete_repo">delete_repo</a></code></li>
<li><code><a title="python_pachyderm.client.Client.finish_commit" href="#python_pachyderm.client.Client.finish_commit">finish_commit</a></code></li>
<li><code><a title="python_pachyderm.client.Client.flush_commit" href="#python_pachyderm.client.Client.flush_commit">flush_commit</a></code></li>
<li><code><a title="python_pachyderm.client.Client.flush_job" href="#python_pachyderm.client.Client.flush_job">flush_job</a></code></li>
<li><code><a title="python_pachyderm.client.Client.garbage_collect" href="#python_pachyderm.client.Client.garbage_collect">garbage_collect</a></code></li>
<li><code><a title="python_pachyderm.client.Client.get_file" href="#python_pachyderm.client.Client.get_file">get_file</a></code></li>
<li><code><a title="python_pachyderm.client.Client.get_job_logs" href="#python_pachyderm.client.Client.get_job_logs">get_job_logs</a></code></li>
<li><code><a title="python_pachyderm.client.Client.get_pipeline_logs" href="#python_pachyderm.client.Client.get_pipeline_logs">get_pipeline_logs</a></code></li>
<li><code><a title="python_pachyderm.client.Client.get_remote_version" href="#python_pachyderm.client.Client.get_remote_version">get_remote_version</a></code></li>
<li><code><a title="python_pachyderm.client.Client.glob_file" href="#python_pachyderm.client.Client.glob_file">glob_file</a></code></li>
<li><code><a title="python_pachyderm.client.Client.inspect_branch" href="#python_pachyderm.client.Client.inspect_branch">inspect_branch</a></code></li>
<li><code><a title="python_pachyderm.client.Client.inspect_commit" href="#python_pachyderm.client.Client.inspect_commit">inspect_commit</a></code></li>
<li><code><a title="python_pachyderm.client.Client.inspect_datum" href="#python_pachyderm.client.Client.inspect_datum">inspect_datum</a></code></li>
<li><code><a title="python_pachyderm.client.Client.inspect_file" href="#python_pachyderm.client.Client.inspect_file">inspect_file</a></code></li>
<li><code><a title="python_pachyderm.client.Client.inspect_job" href="#python_pachyderm.client.Client.inspect_job">inspect_job</a></code></li>
<li><code><a title="python_pachyderm.client.Client.inspect_pipeline" href="#python_pachyderm.client.Client.inspect_pipeline">inspect_pipeline</a></code></li>
<li><code><a title="python_pachyderm.client.Client.inspect_repo" href="#python_pachyderm.client.Client.inspect_repo">inspect_repo</a></code></li>
<li><code><a title="python_pachyderm.client.Client.list_branch" href="#python_pachyderm.client.Client.list_branch">list_branch</a></code></li>
<li><code><a title="python_pachyderm.client.Client.list_commit" href="#python_pachyderm.client.Client.list_commit">list_commit</a></code></li>
<li><code><a title="python_pachyderm.client.Client.list_datum" href="#python_pachyderm.client.Client.list_datum">list_datum</a></code></li>
<li><code><a title="python_pachyderm.client.Client.list_file" href="#python_pachyderm.client.Client.list_file">list_file</a></code></li>
<li><code><a title="python_pachyderm.client.Client.list_job" href="#python_pachyderm.client.Client.list_job">list_job</a></code></li>
<li><code><a title="python_pachyderm.client.Client.list_pipeline" href="#python_pachyderm.client.Client.list_pipeline">list_pipeline</a></code></li>
<li><code><a title="python_pachyderm.client.Client.list_repo" href="#python_pachyderm.client.Client.list_repo">list_repo</a></code></li>
<li><code><a title="python_pachyderm.client.Client.put_file_bytes" href="#python_pachyderm.client.Client.put_file_bytes">put_file_bytes</a></code></li>
<li><code><a title="python_pachyderm.client.Client.put_file_url" href="#python_pachyderm.client.Client.put_file_url">put_file_url</a></code></li>
<li><code><a title="python_pachyderm.client.Client.restart_datum" href="#python_pachyderm.client.Client.restart_datum">restart_datum</a></code></li>
<li><code><a title="python_pachyderm.client.Client.run_pipeline" href="#python_pachyderm.client.Client.run_pipeline">run_pipeline</a></code></li>
<li><code><a title="python_pachyderm.client.Client.start_commit" href="#python_pachyderm.client.Client.start_commit">start_commit</a></code></li>
<li><code><a title="python_pachyderm.client.Client.start_pipeline" href="#python_pachyderm.client.Client.start_pipeline">start_pipeline</a></code></li>
<li><code><a title="python_pachyderm.client.Client.stop_job" href="#python_pachyderm.client.Client.stop_job">stop_job</a></code></li>
<li><code><a title="python_pachyderm.client.Client.stop_pipeline" href="#python_pachyderm.client.Client.stop_pipeline">stop_pipeline</a></code></li>
<li><code><a title="python_pachyderm.client.Client.subscribe_commit" href="#python_pachyderm.client.Client.subscribe_commit">subscribe_commit</a></code></li>
<li><code><a title="python_pachyderm.client.Client.walk_file" href="#python_pachyderm.client.Client.walk_file">walk_file</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>